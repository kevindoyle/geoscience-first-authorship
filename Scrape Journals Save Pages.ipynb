{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "import time\n",
    "\n",
    "import gender\n",
    "from gender import getGenders\n",
    "\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using root directory: /Users/mariak/Documents/GitHub/geoscience-first-authorship\n"
     ]
    }
   ],
   "source": [
    "# Configure local paths\n",
    "\n",
    "root = ! pwd\n",
    "root = root[0]\n",
    "\n",
    "print(\"using root directory:\", root)\n",
    "\n",
    "CHROME_DRIVER_PATH=root+\"/deps/chromedriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse Journal Data.ipynb         \u001b[1m\u001b[34mdeps\u001b[m\u001b[m\r\n",
      "README.md                        gender.py\r\n",
      "Scrape Journals Save Pages.ipynb requirements.txt\r\n",
      "\u001b[1m\u001b[34m__pycache__\u001b[m\u001b[m                      \u001b[1m\u001b[34mtest\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page:\n",
    "    def __init__(self, raw, month, year, journal_id):\n",
    "        self.raw = raw\n",
    "        self.month = month\n",
    "        self.year = year\n",
    "        self.journal_id = journal_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 8] Exec format error: '/Users/mariak/Documents/GitHub/geoscience-first-authorship/deps/chromedriver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a7c1cfaadbd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChromeOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--incognito\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHROME_DRIVER_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             log_path=service_log_path)\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m                                             \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                             \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                             stdin=PIPE)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    754\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1497\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 8] Exec format error: '/Users/mariak/Documents/GitHub/geoscience-first-authorship/deps/chromedriver'"
     ]
    }
   ],
   "source": [
    "# Initialize webdriver\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(\"--incognito\")\n",
    "browser = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH, options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to scrape AGU pages, run a test\n",
    "\n",
    "template_string = \"https://agupubs.onlinelibrary.wiley.com/action/doSearch?field1=AllField&text1=&field2=AllField&text2=&field3=AllField&text3=&publication[]=21698996&Ppub=&AfterMonth={month}&AfterYear={year}&BeforeMonth={month}&BeforeYear={year}&startPage={start_page}&sortBy=Earliest&\"\n",
    "\n",
    "\n",
    "def fetch_page(browser, url):\n",
    "    browser.get(url)\n",
    "    if browser.current_url != url:\n",
    "        print(\"unexpected page url.\\n current: {} \\n expected: {}\".format(browser.current_url,url))\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    # create a fingerprint for this page\n",
    "    titles = []\n",
    "    elements = browser.find_elements_by_class_name(\"publication_title\")\n",
    "    for e in elements:\n",
    "        titles.append(e.text)\n",
    "    fingerprint = \" \".join(titles)\n",
    "\n",
    "    return browser.page_source, fingerprint\n",
    "\n",
    "url = template_string.format(year=\"2013\", month=\"05\", start_page=1)\n",
    "html, fingerprint = fetch_page(browser, url)\n",
    "\n",
    "with codecs.open(\"test/test_page.html\", \"w\", \"utf8\") as outfile:\n",
    "    outfile.write(html)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journals to scrape. [name, template]\n",
    "\n",
    "agu_search_template = \"https://agupubs.onlinelibrary.wiley.com/action/doSearch?field1=AllField&text1=&field2=AllField&text2=&field3=AllField&text3=&publication[]={publication}&Ppub=&AfterMonth={month}&AfterYear={year}&BeforeMonth={month}&BeforeYear={year}&startPage={start_page}&sortBy=Earliest&\"\n",
    "\n",
    "journal_templates = [\n",
    "    [\"JGRAtmosphere\", \"21698996\"],\n",
    "    [\"JGREarthSurface\",\"21699011\"],\n",
    "    [\"GRL\",\"19448007\"],\n",
    "    [\"JGROceans\",\"21699291\"],\n",
    "    [\"JGRSolidEarth\",\"21699356\"],\n",
    "    [\"JGRSpacePhysics\",\"21699402\"],\n",
    "    [\"JGRBioGeoSciences\",\"21698961\"],\n",
    "    [\"JGRPlanets\",\"21699100\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years to collect\n",
    "years = [\n",
    "    \"2013\",\n",
    "    \"2014\",\n",
    "    \"2015\",\n",
    "    \"2016\",\n",
    "    \"2017\",\n",
    "    \"2018\",\n",
    "    \"2019\"\n",
    "]\n",
    "\n",
    "# months to collect\n",
    "months = [\n",
    "    \"01\",\n",
    "    \"02\",\n",
    "    \"03\",\n",
    "    \"04\",\n",
    "    \"05\",\n",
    "    \"06\",\n",
    "    \"07\",\n",
    "    \"08\",\n",
    "    \"09\",\n",
    "    \"10\",\n",
    "    \"11\",\n",
    "    \"12\"\n",
    "]\n",
    "\n",
    "days_in_month = [\n",
    "    \"31\",\n",
    "    \"28\",\n",
    "    \"31\",\n",
    "    \"30\",\n",
    "    \"31\",\n",
    "    \"30\",\n",
    "    \"31\",\n",
    "    \"31\",\n",
    "    \"30\",\n",
    "    \"31\",\n",
    "    \"30\",\n",
    "    \"31\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the pages\n",
    "\n",
    "previous_fingerprint = \"\"\n",
    "\n",
    "\n",
    "for journal_name, journal_id in journal_templates:\n",
    "    print(\"scraping journal:\", journal_name)\n",
    "    for year in years:\n",
    "        print(\"scraping year:\", year)\n",
    "        for month in months:\n",
    "            print(\"scraping month:\", month)\n",
    "            for page in range(1,101):\n",
    "                url = agu_search_template.format(publication=journal_id, year=year, month=month, start_page=page)\n",
    "                html, fingerprint = fetch_page(browser, url)\n",
    "                if len(html) == 0:\n",
    "                    print(\"nothing to save for\", url)\n",
    "                    continue\n",
    "                if fingerprint == previous_fingerprint:\n",
    "                    # page already seen, move to the next month\n",
    "                    break\n",
    "                previous_fingerprint = fingerprint\n",
    "\n",
    "                filename = \"{name}_{year}_{month}_{page}.html\".format(name=journal_name, year=year, month=month, page=page)\n",
    "                with codecs.open(\"pages/\"+filename, \"w\", \"utf8\") as outfile:\n",
    "                    outfile.write(html)\n",
    "\n",
    "                time.sleep(2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoScienceWorld journal values\n",
    "\n",
    "gsw_template = \"https://pubs.geoscienceworld.org/search-results?page={page_number}&f_JournalDisplayName={journal_name}&fl_ContentType=Journal+Article+OR+Journal+OR+Book+OR+Book+Chapter+OR+GeoRef+Record&fl_JournalID={publication}&rg_PublicationDate={month}%2f{day}%2f{year}+TO+{month}%2f{last_day}%2f{year}&restypeid=3&f_ArticleTypeDisplayName=Research+Article\" \n",
    "\n",
    "gsw_journal_values = [\n",
    "    [\"Geology\", \"33\"],\n",
    "    [\"GSA+Bulletin\", \"35\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to scrape GSW pages\n",
    "\n",
    "def fetch_gsw_page(browser, url):\n",
    "    browser.get(url)\n",
    "    if browser.current_url != url:\n",
    "        print(\"unexpected page url.\\n current: {} \\n expected: {}\".format(browser.current_url,url))\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    # create a fingerprint for this page\n",
    "    titles = []\n",
    "    elements = browser.find_elements_by_class_name(\"al-title\")\n",
    "    for e in elements:\n",
    "        titles.append(e.text)\n",
    "    fingerprint = \" \".join(titles)\n",
    "\n",
    "    return browser.page_source, fingerprint\n",
    "\n",
    "url = gsw_template.format(\n",
    "                    journal_name=\"Geology\",\n",
    "                    publication=\"33\",\n",
    "                    year=\"2016\",\n",
    "                    month=\"03\",\n",
    "                    day=\"01\",\n",
    "                    last_day=\"31\",\n",
    "                    page_number=1)\n",
    "html, fingerprint = fetch_gsw_page(browser, url)\n",
    "\n",
    "with codecs.open(\"test/test_gsw_page.html\", \"w\", \"utf8\") as outfile:\n",
    "    outfile.write(html)\n",
    "    \n",
    "print(\"fingerprint:\", fingerprint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape GeoScienceWorld\n",
    "\n",
    "\n",
    "\n",
    "previous_fingerprint = \"\"\n",
    "\n",
    "for journal_name, journal_id in gsw_journal_values:\n",
    "    print(\"scraping journal:\", journal_name)\n",
    "    for year in years:\n",
    "        print(\"scraping year:\", year)\n",
    "        for i, month in enumerate(months):\n",
    "            print(\"scraping month:\", month)\n",
    "            last_day = days_in_month[i]\n",
    "            for page in range(1,101):\n",
    "                url = gsw_template.format(\n",
    "                    journal_name=journal_name,\n",
    "                    publication=journal_id,\n",
    "                    year=year,\n",
    "                    month=month,\n",
    "                    day=\"01\",\n",
    "                    last_day=last_day,\n",
    "                    page_number=page)\n",
    "                html, fingerprint = fetch_gsw_page(browser, url)\n",
    "                if len(html) == 0:\n",
    "                    print(\"nothing to save for\", url)\n",
    "                    continue\n",
    "                if fingerprint == previous_fingerprint:\n",
    "                    # page already seen, move to the next month\n",
    "                    print(\"done on page\", page)\n",
    "                    break\n",
    "                previous_fingerprint = fingerprint\n",
    "\n",
    "                filename = \"{name}_{year}_{month}_{page}.html\".format(name=journal_name, year=year, month=month, page=page)\n",
    "                with codecs.open(\"pages/\"+filename, \"w\", \"utf8\") as outfile:\n",
    "                    outfile.write(html)\n",
    "\n",
    "                time.sleep(2)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup to scrape nature geoscience\n",
    "\n",
    "ngs_template = \"https://www.nature.com/search?article_type=research%2Creviews&date_range={year}-{year}&journal=ngeo&order=relevance&page={page}\" \n",
    "\n",
    "def fetch_ngs_page(browser, url):\n",
    "    browser.get(url)\n",
    "    if browser.current_url != url:\n",
    "        print(\"unexpected page url.\\n current: {} \\n expected: {}\".format(browser.current_url,url))\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    # create a fingerprint for this page\n",
    "    titles = []\n",
    "    elements = browser.find_elements_by_class_name(\"h3\")\n",
    "    for e in elements:\n",
    "        titles.append(e.text)\n",
    "    fingerprint = \" \".join(titles)\n",
    "\n",
    "    return browser.page_source, fingerprint\n",
    "\n",
    "url = ngs_template.format(year=\"2015\", page=1)\n",
    "html, fingerprint = fetch_ngs_page(browser, url)\n",
    "\n",
    "with codecs.open(\"test/test_ngs_page.html\", \"w\", \"utf8\") as outfile:\n",
    "    outfile.write(html)\n",
    "    \n",
    "print(\"fingerprint:\", fingerprint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape nature geoscience\n",
    "\n",
    "for year in years:\n",
    "    print(\"scraping year:\", year)\n",
    "    for page in range(1,101):\n",
    "        url = ngs_template.format(year=year, page=page)\n",
    "        html, fingerprint = fetch_ngs_page(browser, url)\n",
    "        if len(html) == 0:\n",
    "            print(\"nothing to save for\", url)\n",
    "            continue\n",
    "        if fingerprint == previous_fingerprint:\n",
    "            # page already seen, move to the next month\n",
    "            print(\"done on page\", page)\n",
    "            break\n",
    "        previous_fingerprint = fingerprint\n",
    "\n",
    "        filename = \"{name}_{year}_{month}_{page}.html\".format(name=\"NatureGeoscience\", year=year, month=\"0\", page=page)\n",
    "        with codecs.open(\"pages/\"+filename, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(html)\n",
    "\n",
    "        time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for science direct scraping\n",
    "\n",
    "# note: offset is (page-1)*100\n",
    "science_direct_template = \"https://www.sciencedirect.com/search/advanced?pub={journal}&cid={journal_id}&date={year}&articleTypes=REV%2CFLA&show=100&sortBy=relevance&offset={offset}\"\n",
    "\n",
    "sd_journal_values = [\n",
    "    [\"Quaternary%20Science%20Reviews\", \"271861\"],\n",
    "    [\"Geochimica%20et%20Cosmochimica%20Acta\", \"271865\"]\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_sd_page(browser, url):\n",
    "    browser.get(url)\n",
    "    if browser.current_url != url:\n",
    "        print(\"unexpected page url.\\n current: {} \\n expected: {}\".format(browser.current_url,url))\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    # create a fingerprint for this page\n",
    "    titles = []\n",
    "    elements = browser.find_elements_by_class_name(\"result-list-title-link\")\n",
    "    for e in elements:\n",
    "        titles.append(e.text)\n",
    "    fingerprint = \" \".join(titles)\n",
    "\n",
    "    return browser.page_source, fingerprint\n",
    "\n",
    "journal, journal_id = sd_journal_values[0]\n",
    "url = science_direct_template.format(journal=journal, journal_id=journal_id, year=\"2015\", offset=0)\n",
    "html, fingerprint = fetch_sd_page(browser, url)\n",
    "\n",
    "with codecs.open(\"test/test_sd_page.html\", \"w\", \"utf8\") as outfile:\n",
    "    outfile.write(html)\n",
    "    \n",
    "print(\"fingerprint:\", fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape science direct\n",
    "\n",
    "for journal, journal_id in sd_journal_values:\n",
    "    for year in years:\n",
    "        print(\"scraping year:\", year)\n",
    "        for page in range(1,101):\n",
    "            url = science_direct_template.format(journal=journal, journal_id=journal_id, year=year, offset=((page-1)*100))\n",
    "            html, fingerprint = fetch_sd_page(browser, url)\n",
    "            if len(html) == 0:\n",
    "                print(\"nothing to save for\", url)\n",
    "                continue\n",
    "            if fingerprint == previous_fingerprint:\n",
    "                # page already seen, move to the next month\n",
    "                print(\"done on page\", page)\n",
    "                break\n",
    "            previous_fingerprint = fingerprint\n",
    "\n",
    "            filename = \"{name}_{year}_{month}_{page}.html\".format(name=journal, year=year, month=\"0\", page=page)\n",
    "            with codecs.open(\"pages/\"+filename, \"w\", \"utf8\") as outfile:\n",
    "                outfile.write(html)\n",
    "\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from scrape_webjournals\n",
    "\n",
    "\n",
    "template_string = \"https://agupubs.onlinelibrary.wiley.com/action/doSearch?field1=AllField&text1=&field2=AllField&text2=&field3=AllField&text3=&publication[]=21698996&Ppub=&AfterMonth=05&AfterYear={after_year}&BeforeMonth=05&BeforeYear={before_year}&startPage={start_page}&sortBy=Earliest\"\n",
    "\n",
    "names = []\n",
    "\n",
    "names_from_one_page = []\n",
    "names_string = \"the-place-holder\"\n",
    "\n",
    "for year in range(2013,2019):\n",
    "    print(\"scraping year\", year)\n",
    "    for i in range(1,101):\n",
    "        names_from_one_page = []\n",
    "        # BUG: The last result page repeats itself, so we need a different way to detect when\n",
    "        # we've scraped the last page of good results. Either: Find the number of results and\n",
    "        # calculate the number of pages, or compare the last list of names with the current and\n",
    "        # stop when they match.\n",
    "        print(\"page\", i)\n",
    "        browser.get(\n",
    "            template_string.format(\n",
    "                after_year=year,\n",
    "                before_year=year+1,\n",
    "                start_page=i\n",
    "            )\n",
    "        )\n",
    "        author_divs = browser.find_elements_by_class_name(\"meta__authors\")\n",
    "        if len(author_divs) == 0:\n",
    "            print(\"ending on page\", i, \"for year\", year)\n",
    "            break\n",
    "        for c in author_divs:\n",
    "            try:\n",
    "                name = c.find_element_by_class_name(\"hlFld-ContribAuthor\").text\n",
    "                names_from_one_page.append(name)\n",
    "            except:\n",
    "                print(\"failed to find class name:\", repr(c.text))\n",
    "                \n",
    "        if len(names_from_one_page) == 0:\n",
    "            continue\n",
    "    \n",
    "        new_name_string = \"\".join(names_from_one_page)\n",
    "        if names_string == new_name_string:\n",
    "            print(\"ending on page\", i, \"for year\", year, \"because the page repeated\")\n",
    "            break\n",
    "        names_string = new_name_string\n",
    "        \n",
    "        # We check that the page was not repeated before saving any of the \n",
    "        # names in the _names_ list.\n",
    "        for n in names_from_one_page:\n",
    "            names.append(n)\n",
    "            \n",
    "        print(\"found names so far:\", len(names))\n",
    "        time.sleep(2)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
