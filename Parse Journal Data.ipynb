{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "import gender\n",
    "from gender import getGenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure local paths\n",
    "\n",
    "root = ! pwd\n",
    "root = root[0]\n",
    "\n",
    "print(\"using root directory:\", root)\n",
    "\n",
    "RAW_PAGES_DIR=root+\"/pages/\"\n",
    "PARSED_PAGES_DIR=root+\"/parsed/\"\n",
    "GUESSED_NAMES_DIR=root+\"/guessed/\"\n",
    "NAME_GENDER_DIR=root+\"/name_genders/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PARSED_PAGES_DIR):\n",
    "    os.mkdir(PARSED_PAGES_DIR)\n",
    "    \n",
    "if not os.path.exists(GUESSED_NAMES_DIR):\n",
    "    os.mkdir(GUESSED_NAMES_DIR)\n",
    "    \n",
    "if not os.path.exists(NAME_GENDER_DIR):\n",
    "    os.mkdir(NAME_GENDER_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and define useful functions and\n",
    "# data structures.\n",
    "\n",
    "def is_initialed_name(name):\n",
    "    first_term = name.split(\" \")[0]\n",
    "    if len(first_term) == 0:\n",
    "        # print(\"first term len zero. name:\", name)\n",
    "        return False\n",
    "    return first_term[-1] == \".\" and first_term[:-1].isupper()\n",
    "\n",
    "print(\"test is_initialed_name- True:\", is_initialed_name(\"J. Smith\"), \", False:\", is_initialed_name(\"Joe Smith\"))\n",
    "\n",
    "def contains_initialed_name(names):\n",
    "    for name in names:\n",
    "        if is_initialed_name(name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"test contains_initialed_name- True\", contains_initialed_name([\"J. Smith\", \"Cat Meowins\"]), \", False:\", contains_initialed_name([\"Joe Smith\", \"Cat Meowins\"]))\n",
    "\n",
    "def clean_name(name):\n",
    "    name = name.strip()\n",
    "    name = re.sub(r'\\s', ' ', name)\n",
    "    if not is_initialed_name(name):\n",
    "        terms = name.split(\" \")\n",
    "        terms[0] = terms[0].strip(\".\")\n",
    "        name = \" \".join(terms)\n",
    "    return name.strip('.')\n",
    "\n",
    "print(\"test clean name- Colin. J. Cats:\", clean_name(\"Colin. J. Cats\"), \"W. B. Easy:\", clean_name(\"W. B. Easy\"))\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, first_author, all_names, year, month, title, journal):\n",
    "        # clean the author names\n",
    "        # - remove non-ascii whitespace\n",
    "        # - strip bookend whitespace\n",
    "        # - strip periods from first names if not an initialed name\n",
    "        \n",
    "        \n",
    "        self.first_author = clean_name(first_author)\n",
    "        self.names = [clean_name(name) for name in all_names]\n",
    "        self.year = year\n",
    "        self.month = month\n",
    "        self.title = title\n",
    "        self.journal = journal\n",
    "        \n",
    "        # create a unique identifier for this article\n",
    "        self.id = \"_\".join([journal, year, month, \"_\".join(title.translate(str.maketrans('', '', string.punctuation)).split(\" \"))])\n",
    "        \n",
    "        # determine if article has initialed names\n",
    "        self.has_initials = is_initialed_name(first_author)\n",
    "    \n",
    "    def last_name_set(self):\n",
    "        # return a set() of all the last names\n",
    "        name_set = set()\n",
    "        for name in self.names:\n",
    "            name_set.add(name.split(\" \")[-1])\n",
    "        return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"has_initials\"] = self.has_initials\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def article_from_map(article_map):\n",
    "    return Article(\n",
    "        first_author=article_map[\"first_author\"],\n",
    "        all_names=article_map[\"all_names\"],\n",
    "        year=article_map[\"year\"],\n",
    "        month=article_map[\"month\"],\n",
    "        title=article_map[\"title\"],\n",
    "        journal=article_map[\"journal\"])\n",
    "\n",
    "test_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"A. Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2019\",\n",
    "    month=\"02\",\n",
    "    title=\"a story of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "\n",
    "print(\"last name set:\", test_article.last_name_set())\n",
    "print(\"article id:\", test_article.id)\n",
    "print(\"article has initial:\", test_article.has_initials)\n",
    "print(\"article map:\", test_article.to_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page parse functions\n",
    "\n",
    "def get_parse_function(filename):\n",
    "    if filename.startswith(\"JGR\") or filename.startswith(\"GRL\"):\n",
    "        return parse_agu_page\n",
    "    if filename.startswith(\"Seismological+Research+Letters\") or filename.startswith(\"Bulletin+of+the+Seismological+Society+of+America\"):\n",
    "        return parse_gsw_page\n",
    "    if filename.startswith(\"NatureGeoscience\") or filename.startswith(\"Nature\"):\n",
    "        return parse_ng_page\n",
    "    if filename.startswith(\"Earth+and+Planetary+Science+Letters\") or filename.startswith(\"Physics+of+the+Earth+and+Planetary+Interiors\") or filename.startswith(\"Tectonophysics\"):\n",
    "        return parse_sd_page\n",
    "    if filename.startswith(\"GJI\"):\n",
    "        return parse_gji_page\n",
    " #   if filename.startswith(\"SolidEarth\"):\n",
    "  #      return parse_solidearth_page\n",
    "    if filename.startswith(\"GEOPHYSICS\"):\n",
    "        return parse_geophysics_page\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def parse_agu_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"item__body\")\n",
    "    for a in articles:\n",
    "        meta_title = a.find_all(class_=\"meta__title\")\n",
    "        title = meta_title[0].find_all(\"a\", class_=\"publication_title\")\n",
    "        #title = title[0].string # This doesn not work well with special characters in the title\n",
    "        title = title[0].get_text()\n",
    "\n",
    "        if title == \"Issue Information\":\n",
    "            print('hit \"Issue Information\"')\n",
    "            continue\n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title\")\n",
    "            continue\n",
    "\n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"a\", class_=\"publication_contrib_author\")\n",
    "        for p in author_spans:\n",
    "            author_span = p.span\n",
    "            if author_span.i is not None:\n",
    "                author_span.i.decompose()\n",
    "            authors.append(author_span.string)\n",
    "\n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "\n",
    "        article = Article(\n",
    "                first_author=authors[0],\n",
    "                all_names=authors,\n",
    "                year=year,\n",
    "                month=month,\n",
    "                title=title,\n",
    "                journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "\n",
    "def parse_gsw_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"al-article-box\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"al-title\")\n",
    "        title = title[0].find_all(\"a\")[0].string\n",
    "        \n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title\")\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"wi-fullname\")\n",
    "        for author in author_spans:\n",
    "            author = author.find_all(\"a\")[0].string\n",
    "            authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_ng_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"li\", class_=\"pb20\")\n",
    "    print(\"num articles:\", len(articles))\n",
    "    for a in articles:\n",
    "        title_section = a.find_all(\"h2\", class_=\"h3\")\n",
    "        if title_section[0] is None:\n",
    "            print(\"title section is none:\", a)\n",
    "            continue\n",
    "        title = title_section[0].a.contents[0]\n",
    "        \n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title\")\n",
    "            continue\n",
    "        title = title.strip()\n",
    "        \n",
    "        authors = []\n",
    "        author_span = a.find_all(\"ul\", class_=\"js-list-authors-3\")\n",
    "        for auths in author_span:\n",
    "            for author in auths.find_all(\"a\", class_=\"js-no-scroll\"):\n",
    "                author = author.contents[0]\n",
    "                \n",
    "                if author == \"[…]\" or \"Show fewer authors\" in author:\n",
    "                    continue\n",
    "                    \n",
    "                authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "        \n",
    "    return parsed_articles\n",
    "\n",
    "        \n",
    "def parse_sd_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"li\", class_=\"ResultItem\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(\"a\", class_=\"result-list-title-link\")\n",
    "        title = title[0].contents[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = title.string\n",
    "        if isinstance(title, Tag):\n",
    "            title = title.get_text()\n",
    "\n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            print(\"title:\", a.find_all(\"a\", class_=\"result-list-title-link\"))\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title\")\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"author\")\n",
    "        for author in author_spans:\n",
    "            author = author.string\n",
    "            authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "def parse_gji_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"al-article-box\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"al-title\")\n",
    "        title = title[0].find_all(\"a\")[0].string\n",
    "        \n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title\")\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"div\", class_=\"al-authors-list\")\n",
    "        for author in author_spans:\n",
    "            author = author.find_all(\"a\")[0].string\n",
    "            authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "def parse_solidearth_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"grid-85\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(\"a\",class_=\"article-title\")\n",
    "        title = title[0].find_all(\"a\")[0].string\n",
    "\n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title\")\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(class_=\"al-authors-list\")\n",
    "        for author in author_spans:\n",
    "            author = author.find_all(\"a\")[0].string\n",
    "            authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "def parse_geophysics_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"issue-item__body\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"issue-item__title\")\n",
    "        title = title[0].find_all(\"a\")[0].string\n",
    "        \n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title\")\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"div\", class_=\"issue-item__authors\")\n",
    "        for author in author_spans:\n",
    "            author = author.find_all(\"a\")[0].string\n",
    "            authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk directory, parse, and save parsed articles\n",
    "\n",
    "for _, _, files in os.walk(RAW_PAGES_DIR):\n",
    "    for file in files:\n",
    "        \n",
    "#         # CHANGE THIS TO FILTER FOR SPECIFIC JOURNALS\n",
    "#         if not (file.startswith(\"Quaternary\") or file.startswith(\"Geochimica\")): \n",
    "#             continue\n",
    "        \n",
    "        print(\"processing file:\", file)\n",
    "        journal, year, month, _ = file.split(\"_\")\n",
    "\n",
    "        html = \"\"\n",
    "        with codecs.open(RAW_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            html = infile.read()\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        parser = get_parse_function(file)\n",
    "        if parser is None:\n",
    "            print(\"got None parse function, skipping file:\", file)\n",
    "            continue\n",
    "        \n",
    "        for article in parser(soup, year, month, journal):\n",
    "            outfile_name = article.id[:80]+\".json\"\n",
    "            with codecs.open(PARSED_PAGES_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "                outfile.write(json.dumps(article.to_map()))\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create in-memory map for name guessing\n",
    "\n",
    "# This map is generated from the articles where names are not initialed.\n",
    "# The keys on the map are last names. The values are arrays of articles\n",
    "# where at least one author on the article has the keyed last name.\n",
    "last_names_to_articles = {}\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if art.has_initials:\n",
    "            continue\n",
    "        last_name = art.first_author.split(\" \")[-1]\n",
    "        if last_name not in last_names_to_articles.keys():\n",
    "            last_names_to_articles[last_name] = []\n",
    "        last_names_to_articles[last_name].append(art)\n",
    "        \n",
    "print(\"last name map size\", len(last_names_to_articles.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Guessed:\n",
    "    def __init__(self, primary_article, guessed_name, match_article_id):\n",
    "        self.first_author = primary_article.first_author\n",
    "        self.names = primary_article.names\n",
    "        self.year = primary_article.year\n",
    "        self.month = primary_article.month\n",
    "        self.title = primary_article.title\n",
    "        self.journal = primary_article.journal\n",
    "        \n",
    "        # create a unique identifier for this article\n",
    "        self.id = \"_\".join([self.journal, self.year, self.month, \"_\".join(self.title.translate(str.maketrans('', '', string.punctuation)).split(\" \"))])\n",
    "        \n",
    "        # determine if article has initialed names\n",
    "        self.has_initials = is_initialed_name(self.first_author)\n",
    "        \n",
    "        self.guessed_name = guessed_name\n",
    "        self.match_article_id = match_article_id\n",
    "    \n",
    "    def last_name_set(self):\n",
    "        # return a set() of all the last names\n",
    "        name_set = set()\n",
    "        for name in self.names:\n",
    "            name_set.add(name.split(\" \")[-1])\n",
    "        return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"has_initials\"] = self.has_initials\n",
    "        m[\"guessed_name\"] = self.guessed_name\n",
    "        m[\"match_article_id\"] = self.match_article_id\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def guessed_from_map(guessed_map):\n",
    "    g = Guessed(\n",
    "        primary_article=article_from_map(guessed_map),\n",
    "        guessed_name=guessed_map[\"guessed_name\"],\n",
    "        match_article_id=guessed_map[\"match_article_id\"])\n",
    "    return g\n",
    "\n",
    "test_primary_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"A. Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2019\",\n",
    "    month=\"02\",\n",
    "    title=\"a story of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "test_match_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"Arthur Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2018\",\n",
    "    month=\"05\",\n",
    "    title=\"existence of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "test_guessed = Guessed(test_primary_article, \"Arthur Cat\", test_match_article.id)\n",
    "\n",
    "print(test_guessed.to_map())\n",
    "print(guessed_from_map(test_guessed.to_map()).to_map() == test_guessed.to_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many documents have initialed first authors\n",
    "count = 0\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if not art.has_initials:\n",
    "            continue\n",
    "        count += 1\n",
    "        \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create guesses for first author names and save to files\n",
    "\n",
    "def extract_name_guess(initial_name, possible_names):\n",
    "    last_name = initial_name.split(\" \")[-1]\n",
    "    for pn in possible_names:\n",
    "        if last_name != pn.split(\" \")[-1]:\n",
    "            continue  # not the name we're looking for\n",
    "        if initial_name[0] != pn[0]:\n",
    "            continue  # first letters do not match\n",
    "        if is_initialed_name(pn):\n",
    "            continue\n",
    "        return pn, True\n",
    "    return \"\", False\n",
    "\n",
    "\n",
    "# unsupported edge cases\n",
    "#\n",
    "# Check for hyphenated name\n",
    "# X.‐J. Zhang => Xuanze Zhang\n",
    "# X.‐J. Zhang => Xiao‐Jia Zhang\n",
    "# X.‐J. Zhang => Xu Zhang\n",
    "# X.‐J. Zhang => Xiaojia Zhang\n",
    "# X.‐J. Zhang => Xiaohe Zhang\n",
    "#\n",
    "# Check that the same initialized name\n",
    "# is not mapped to multiple different\n",
    "# complete names\n",
    "# J. M. Li => Jia Li\n",
    "# J. M. Li => Jiaxun Li\n",
    "# J. M. Li => Jingyuan Li\n",
    "\n",
    "\n",
    "unmapped_names = set()\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if not art.has_initials:\n",
    "            continue\n",
    "        last_name = art.first_author.split(\" \")[-1]\n",
    "        if last_name not in last_names_to_articles.keys():\n",
    "            unmapped_names.add(art.first_author)\n",
    "            continue\n",
    "        articles = last_names_to_articles[last_name]\n",
    "        \n",
    "        # gather all the guesses, paired with the size of overlap.\n",
    "        # guesses with largest overlap will be written to file. \n",
    "        guesses = []\n",
    "        for article in articles:\n",
    "            overlap = article.last_name_set() & art.last_name_set()\n",
    "            if len(overlap) < 2:\n",
    "                continue  # skip articles without enough overlap\n",
    "            name, ok = extract_name_guess(art.first_author, article.names)\n",
    "            if not ok:\n",
    "                continue\n",
    "                \n",
    "            guessed = Guessed(art, name, article.id)\n",
    "            guesses.append((len(overlap), guessed))\n",
    "            \n",
    "        if len(guesses) == 0:\n",
    "            continue\n",
    "            \n",
    "        sorted(guesses, key=lambda x: x[0], reverse=True)\n",
    "        the_guess = guesses[0][1]\n",
    "             \n",
    "        outfile_name = the_guess.id[:80]+\".json\"\n",
    "        with codecs.open(GUESSED_NAMES_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(the_guess.to_map()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unmapped_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect list of all names to be gendered\n",
    "\n",
    "guessed_first_names = set()\n",
    "\n",
    "for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            guess = guessed_from_map(json.loads(infile.read()))\n",
    "        first_name = guess.guessed_name.split(\" \")[0]\n",
    "        if len(first_name) == 0:\n",
    "            print(\"empty first name?:\", guess.guessed_name)\n",
    "            continue\n",
    "        guessed_first_names.add(first_name)\n",
    "        \n",
    "print(\"number of unique first names to guess:\", len(guessed_first_names))\n",
    "guessed_first_names\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the gender api\n",
    "\n",
    "class GenderResult(object):\n",
    "    def __init__(self, name, result):\n",
    "        self.name = name\n",
    "        self.binary = result[0]\n",
    "        self.percent = result[1]\n",
    "        self.count = result[2]\n",
    "        \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"name\"] = self.name\n",
    "        m[\"binary\"] = self.binary\n",
    "        m[\"percent\"] = self.percent\n",
    "        m[\"count\"] = self.count\n",
    "        return m\n",
    "    \n",
    "\n",
    "def gender_result_from_map(m):\n",
    "    return GenderResult(m[\"name\"], ( m[\"binary\"], m[\"percent\"], m[\"count\"]))\n",
    "\n",
    "name = \"Taylor\"\n",
    "g = getGenders(name)[0]\n",
    "gr = GenderResult(name, g)\n",
    "gender_result_from_map(json.loads(json.dumps(gr.to_map()))).to_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Gender API and save output for all the names\n",
    "# WARN: Makes API calls\n",
    "\n",
    "for name in guessed_first_names:\n",
    "    result = getGenders(name)\n",
    "    if len(result) > 1:\n",
    "        print(\"long result:\", result)\n",
    "    r = result[0]\n",
    "    gr = GenderResult(name, r)\n",
    "    \n",
    "    file_name = \"_\".join(name.translate(str.maketrans('', '', string.punctuation)).split(\" \")) + \".json\"\n",
    "    with codecs.open(NAME_GENDER_DIR+file_name, \"w\", \"utf8\") as outfile:\n",
    "        outfile.write(json.dumps(gr.to_map()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Gender API gender guesses into memory\n",
    "\n",
    "gender_results = []\n",
    "\n",
    "for _, _, files in os.walk(NAME_GENDER_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(NAME_GENDER_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            gr = gender_result_from_map(json.loads(infile.read()))\n",
    "            gender_results.append(gr)\n",
    "            \n",
    "len(gender_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map of first name to gender result\n",
    "\n",
    "name_to_gr = {}\n",
    "for gr in gender_results:\n",
    "    name_to_gr[gr.name] = gr\n",
    "    \n",
    "len(name_to_gr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Colin.\" in name_to_gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist in the guessed names\n",
    "# NOTE: This ration should be for all guesses, not the reduced set() of names\n",
    "\n",
    "binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "\n",
    "for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "    for file in files:\n",
    "#         if not file.startswith(\"JGROceans\"):\n",
    "#             continue\n",
    "        with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            guess = guessed_from_map(json.loads(infile.read()))\n",
    "        first_name = guess.guessed_name.split(\" \")[0]\n",
    "        if len(first_name) == 0:\n",
    "            print(\"empty first name?:\", guess.guessed_name)\n",
    "            continue\n",
    "        try:\n",
    "            gr = name_to_gr[first_name]\n",
    "            binary_ratio_first_names[gr.binary] += 1\n",
    "        except KeyError:\n",
    "            print(\"not found in map:\", first_name)\n",
    "        \n",
    "        \n",
    "binary_ratio_first_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of first names from all scraped journals\n",
    "\n",
    "all_first_names = set()\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if art.has_initials:\n",
    "            continue\n",
    "        first_name = art.first_author.split(\" \")[0]\n",
    "        all_first_names.add(first_name)\n",
    "        \n",
    "        \n",
    "print(\"first name set size\", len(all_first_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the gender API for all the first names. Deduped against\n",
    "# existing files in the map name_to_gr. Be sure to run the cell\n",
    "# that loads that map before running this function. When properly\n",
    "# loaded, this cell can be re-run until all names are handled.\n",
    "\n",
    "print(\"starting size:\", len(name_to_gr))\n",
    "\n",
    "handled = 0\n",
    "\n",
    "for name in all_first_names:\n",
    "    if name in name_to_gr:\n",
    "        continue\n",
    "        \n",
    "    handled += 1\n",
    "    \n",
    "    result = getGenders(name)\n",
    "    if len(result) > 1:\n",
    "        print(\"long result:\", result)\n",
    "    r = result[0]\n",
    "    gr = GenderResult(name, r)\n",
    "    \n",
    "    file_name = \"_\".join(name.translate(str.maketrans('', '', string.punctuation)).split(\" \")) + \".json\"\n",
    "    with codecs.open(NAME_GENDER_DIR+file_name, \"w\", \"utf8\") as outfile:\n",
    "        outfile.write(json.dumps(gr.to_map()))\n",
    "        \n",
    "    name_to_gr[name] = gr\n",
    "    if handled > 970:\n",
    "        print(\"hit request limit\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "print(\"finish size:\", len(name_to_gr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(name_to_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist in the guessed names\n",
    "# printed out for each journal.\n",
    "\n",
    "\n",
    "journal_names = [\n",
    "\"Tectonophysics\",\n",
    "\"Physics of the Earth and Planetary Interiors\",\n",
    "\"Earth and Planetary Science Letters\",\n",
    "\"SolidEarth\",\n",
    "\"GEOPHYSICS\",\n",
    "\"NatureGeoscience\",\n",
    "\"Geology\",\n",
    "\"GSA+Bulletin\",\n",
    "\"JGRAtmosphere\",\n",
    "\"JGREarthSurface\",\n",
    "\"GRL\",\n",
    "\"JGROceans\",\n",
    "\"JGRSolidEarth\",\n",
    "\"JGRSpacePhysics\",\n",
    "\"JGRBioGeoSciences\",\n",
    "\"JGRPlanets\",\n",
    "\"GJI\",\n",
    "\"Nature\",\n",
    "\"Bulletin+of+the+Seismological+Society+of+America\",\n",
    "\"Seismological+Research+Letters\"]\n",
    "\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "    \n",
    "    for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                guess = guessed_from_map(json.loads(infile.read()))\n",
    "            first_name = guess.guessed_name.split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", guess.guessed_name)\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                binary_ratio_first_names[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "    print(journal, binary_ratio_first_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist for the un-initialed names\n",
    "# printed out for each journal.\n",
    "\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "    \n",
    "    initialed_name_count = 0\n",
    "    \n",
    "    for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                article = article_from_map(json.loads(infile.read()))\n",
    "            first_name = article.first_author.split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", article.first_author)\n",
    "                continue\n",
    "            if is_initialed_name(first_name):\n",
    "                initialed_name_count += 1\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                binary_ratio_first_names[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "    print(journal, binary_ratio_first_names, \"initialed name count:\", initialed_name_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but split into year/month bins\n",
    "#\n",
    "# Determine how many male/female/unknown names exist for the un-initialed names\n",
    "# printed out for each journal.\n",
    "\n",
    "journal_tallys = {}\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0, 'initialed':0}\n",
    "    \n",
    "    initialed_name_count = 0\n",
    "    \n",
    "    for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                article = article_from_map(json.loads(infile.read()))\n",
    "                \n",
    "            tally = journal_tallys.setdefault(journal, {}).setdefault(article.year, {}).setdefault(article.month, {'male':0, 'female':0, 'None':0, 'initialed':0})\n",
    "            \n",
    "            first_name = article.first_author.strip().split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", article.first_author)\n",
    "                continue\n",
    "            if is_initialed_name(first_name):\n",
    "                tally['initialed'] += 1\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                tally[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "for journal, values in journal_tallys.items():\n",
    "    print(journal, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
