{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "import gender\n",
    "from gender import getGenders\n",
    "\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "import time\n",
    "import openapi_client # pip install git+https://github.com/namsor/namsor-python-sdk2.git\n",
    "from openapi_client.rest import ApiException\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure local paths\n",
    "\n",
    "root = ! pwd\n",
    "root = root[0]\n",
    "\n",
    "print(\"using root directory:\", root)\n",
    "\n",
    "RAW_PAGES_DIR=root+\"/pages/\"\n",
    "PARSED_PAGES_DIR=root+\"/parsed/\"\n",
    "GUESSED_NAMES_DIR=root+\"/guessed/\"\n",
    "NAME_GENDER_DIR=root+\"/name_genders/\"\n",
    "\n",
    "# create directories if they do not exist\n",
    "\n",
    "for d in [RAW_PAGES_DIR,\n",
    "         PARSED_PAGES_DIR,\n",
    "         GUESSED_NAMES_DIR,\n",
    "         NAME_GENDER_DIR]:\n",
    "    try:\n",
    "        os.mkdir(d)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rm $GUESSED_NAMES_DIR*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article class, Guessed class, and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and define useful functions and\n",
    "# data structures.\n",
    "\n",
    "def is_initialed_name(name):\n",
    "    first_term = name.split(\" \")[0]\n",
    "    if len(first_term) == 0:\n",
    "        # print(\"first term len zero. name:\", name)\n",
    "        return False\n",
    "    return first_term[-1] == \".\" and first_term[:-1].isupper() or len(first_term) == 1 or len(first_term.split(\"-\")[0]) == 1\n",
    "\n",
    "\n",
    "print(\"test is_initialed_name- True:\", is_initialed_name(\"J. Smith\"), \", False:\", is_initialed_name(\"Joe Smith\"),\n",
    "     \", True:\", is_initialed_name(\"J Smith\"), \", True:\", is_initialed_name(\"J-P Ampuero\"))\n",
    "\n",
    "def contains_initialed_name(names):\n",
    "    for name in names:\n",
    "        if is_initialed_name(name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"test contains_initialed_name- True\", contains_initialed_name([\"J. Smith\", \"Cat Meowins\"]), \", False:\", contains_initialed_name([\"Joe Smith\", \"Cat Meowins\"]))\n",
    "\n",
    "def clean_name(name):\n",
    "    name = name.strip()\n",
    "    name = re.sub(r'\\s', ' ', name)\n",
    "    if not is_initialed_name(name):\n",
    "        terms = name.split(\" \")\n",
    "        terms[0] = terms[0].strip(\".\")\n",
    "        name = \" \".join(terms)\n",
    "    return name.strip('.')\n",
    "\n",
    "print(\"test clean name- Colin. J. Cats:\", clean_name(\"Colin. J. Cats\"), \"W. B. Easy:\", clean_name(\"W. B. Easy\"))\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, first_author, all_names, year, month, title, journal):\n",
    "        # clean the author names\n",
    "        # - remove non-ascii whitespace\n",
    "        # - strip bookend whitespace\n",
    "        # - strip periods from first names if not an initialed name\n",
    "        \n",
    "        \n",
    "        self.first_author = clean_name(first_author)\n",
    "        self.names = [clean_name(name) for name in all_names]\n",
    "        self.year = year\n",
    "        self.month = month\n",
    "        self.title = title\n",
    "        self.journal = journal\n",
    "        \n",
    "        # create a unique identifier for this article\n",
    "        self.id = \"_\".join([journal, year, month, \"_\".join(title.translate(str.maketrans('', '', string.punctuation)).split(\" \"))])\n",
    "        \n",
    "        # determine if article has initialed names\n",
    "        self.has_initials = is_initialed_name(first_author)\n",
    "    \n",
    "    def last_name_set(self):\n",
    "        # return a set() of all the last names\n",
    "        name_set = set()\n",
    "        for name in self.names:\n",
    "            name_set.add(name.split(\" \")[-1])\n",
    "        return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"has_initials\"] = self.has_initials\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def article_from_map(article_map):\n",
    "    return Article(\n",
    "        first_author=article_map[\"first_author\"],\n",
    "        all_names=article_map[\"all_names\"],\n",
    "        year=article_map[\"year\"],\n",
    "        month=article_map[\"month\"],\n",
    "        title=article_map[\"title\"],\n",
    "        journal=article_map[\"journal\"])\n",
    "\n",
    "test_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"A. Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2019\",\n",
    "    month=\"02\",\n",
    "    title=\"a story of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "\n",
    "print(\"last name set:\", test_article.last_name_set())\n",
    "print(\"article id:\", test_article.id)\n",
    "print(\"article has initial:\", test_article.has_initials)\n",
    "print(\"article map:\", test_article.to_map())\n",
    "\n",
    "\n",
    "class Guessed:\n",
    "    def __init__(self, primary_article, guessed_names, match_article_id):\n",
    "        self.first_author = primary_article.first_author\n",
    "        self.names = primary_article.names\n",
    "        self.year = primary_article.year\n",
    "        self.month = primary_article.month\n",
    "        self.title = primary_article.title\n",
    "        self.journal = primary_article.journal\n",
    "        \n",
    "        # create a unique identifier for this article\n",
    "        self.id = \"_\".join([self.journal, self.year, self.month, \"_\".join(self.title.translate(str.maketrans('', '', string.punctuation)).split(\" \"))])\n",
    "        \n",
    "        # determine if article has initialed names\n",
    "        self.has_initials = True in [is_initialed_name(n) for n in self.names]\n",
    "        \n",
    "        self.guessed_names = guessed_names\n",
    "        self.match_article_id = match_article_id\n",
    "    \n",
    "    \n",
    "    def last_name_set(self):\n",
    "        # return a set() of all the last names\n",
    "        name_set = set()\n",
    "        for name in self.names:\n",
    "            name_set.add(name.split(\" \")[-1])\n",
    "        return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"has_initials\"] = self.has_initials\n",
    "        m[\"guessed_names\"] = self.guessed_names\n",
    "        m[\"match_article_id\"] = self.match_article_id\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def guessed_from_map(guessed_map):\n",
    "    g = Guessed(\n",
    "        primary_article=article_from_map(guessed_map),\n",
    "        guessed_names=guessed_map[\"guessed_names\"],\n",
    "        match_article_id=guessed_map[\"match_article_id\"])\n",
    "    return g\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Gendered:\n",
    "    def __init__(self, primary_article, all_genders, all_percent):\n",
    "        self.first_author = primary_article.first_author\n",
    "        \n",
    "        try:\n",
    "            self.names = primary_article.guessed_names\n",
    "        except:\n",
    "            self.names = primary_article.names\n",
    "        \n",
    "        self.year = primary_article.year\n",
    "        self.month = primary_article.month\n",
    "        self.title = primary_article.title\n",
    "        self.journal = primary_article.journal\n",
    "        self.id = primary_article.id\n",
    "        \n",
    "        # gendered:\n",
    "        self.all_genders = all_genders\n",
    "        self.all_percent = all_percent\n",
    "    \n",
    "#     def last_name_set(self):\n",
    "#         # return a set() of all the last names\n",
    "#         name_set = set()\n",
    "#         for name in self.names:\n",
    "#             name_set.add(name.split(\" \")[-1])\n",
    "#         return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"all_genders\"] = self.all_genders\n",
    "        m[\"all_percent\"] = self.all_percent\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def gendered_from_map(dictfromjson):\n",
    "    \n",
    "    # is the primary article guessed authors?\n",
    "    # if yes:\n",
    "    # use guessed_from_map\n",
    "    # if not:\n",
    "    # use article_from_map\n",
    "    if \"guessed_names\" in list(dictfromjson.keys()):\n",
    "        prim_art = guessed_from_map(dictfromjson)\n",
    "    else:\n",
    "        prim_art = article_from_map(dictfromjson)\n",
    "    \n",
    "    g = Gendered(\n",
    "        primary_article=prim_art,\n",
    "        all_genders=dictfromjson[\"all_genders\"],\n",
    "        all_percent=dictfromjson[\"all_percent\"])\n",
    "    return g\n",
    "\n",
    "\n",
    "test_primary_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"A. Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2019\",\n",
    "    month=\"02\",\n",
    "    title=\"a story of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "test_match_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"Arthur Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2018\",\n",
    "    month=\"05\",\n",
    "    title=\"existence of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "test_guessed = Guessed(test_primary_article, \"Arthur Cat\", test_match_article.id)\n",
    "\n",
    "\n",
    "\n",
    "test_gendered_article = Gendered(\n",
    "    test_primary_article,\n",
    "    all_genders=[\"init\", \"male\", None, \"female\"],\n",
    "    all_percent=[0.0, 0.6, None, 0.5]\n",
    ")\n",
    "print(test_guessed.to_map())\n",
    "print(guessed_from_map(test_guessed.to_map()).to_map() == test_guessed.to_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define page parse functions, specific for each journal\n",
    "\n",
    "def get_parse_function(filename):\n",
    "    if filename.startswith(\"JGR\") or filename.startswith(\"GRL\") or filename.startswith(\"G3\"):\n",
    "        return parse_agu_page\n",
    "    if filename.startswith(\"Seismological+Research+Letters\") or filename.startswith(\"Bulletin+of+the+Seismological+Society+of+America\"):\n",
    "        return parse_gsw_page\n",
    "    if filename.startswith(\"NatureGeoscience\") or filename.startswith(\"Nature\"):\n",
    "        return parse_ng_page\n",
    "    if filename.startswith(\"E%26PSL\") or filename.startswith(\"PEPI\") or filename.startswith(\"Tectp\"):\n",
    "        return parse_sd_page\n",
    "    if filename.startswith(\"GJI\"):\n",
    "        return parse_gji_page\n",
    "    if filename.startswith(\"SolidEarth\"):\n",
    "        return parse_solidearth_page\n",
    "    if filename.startswith(\"GEOPHYSICS\"):\n",
    "        return parse_geophysics_page\n",
    "    if filename.startswith(\"Science\"):\n",
    "        return parse_science_page\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_agu_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"item__body\")\n",
    "    for a in articles:\n",
    "        meta_title = a.find_all(class_=\"meta__title\")\n",
    "        title = meta_title[0].find_all(\"a\", class_=\"publication_title\")\n",
    "        title = title[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "       \n",
    "        if title == \"Issue Information\":\n",
    "            print('hit \"Issue Information\"')\n",
    "            continue\n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "\n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"a\", class_=\"publication_contrib_author\")\n",
    "        for p in author_spans:\n",
    "            author_span = p.span\n",
    "            if author_span.i is not None:\n",
    "                author_span.i.decompose()\n",
    "            authors.append(str(author_span.string))\n",
    "   \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "\n",
    "        article = Article(\n",
    "                first_author=authors[0],\n",
    "                all_names=authors,\n",
    "                year=year,\n",
    "                month=month,\n",
    "                title=title,\n",
    "                journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "\n",
    "def parse_gsw_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"al-article-box\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"al-title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"wi-fullname\")\n",
    "        for author in author_spans:\n",
    "            author = str(author.find_all(\"a\")[0].get_text())\n",
    "            authors.append(author)\n",
    "          \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_ng_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"li\", class_=\"pb20\")\n",
    "\n",
    "    for a in articles:\n",
    "        title_section = a.find_all(\"h2\", class_=\"h3\")\n",
    "        if title_section[0] is None:\n",
    "            print(\"title section is none:\", a)\n",
    "            continue\n",
    "        title = title_section[0].a.contents[0]\n",
    "        \n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "        title = title.strip()\n",
    "        \n",
    "        \n",
    "        authors = []\n",
    "        author_span = a.find_all(\"ul\", class_=\"js-list-authors-3\")\n",
    "        for auths in author_span:\n",
    "            for author in auths.find_all(\"a\", class_=\"js-no-scroll\"):\n",
    "                author = author.contents[0]\n",
    "                \n",
    "                if author == \"[…]\" or \"Show fewer authors\" in author:\n",
    "                    continue\n",
    "                \n",
    "                authors.append(author)\n",
    "                \n",
    "                \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "        \n",
    "    return parsed_articles\n",
    "\n",
    "        \n",
    "def parse_sd_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"col-sm-12\")\n",
    "\n",
    "    for a in articles:\n",
    "        title = a.find_all(\"h3\", class_=\"s-results-title\")\n",
    "        if title == \"None\":\n",
    "            #print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            #print(\"hit empty title\")\n",
    "            continue\n",
    "        title = title[0]\n",
    "        title = str(title.get_text())\n",
    "\n",
    "        authors = []\n",
    "        author_field = a.find(\"ul\", class_=\"all-authors\")\n",
    "        if author_field is None:\n",
    "            continue\n",
    "        author_list = author_field.find_all(\"li\", class_=\"article-author\")\n",
    "        for p in author_list:\n",
    "            author = p.get_text().strip(\";\")\n",
    "            name = author.split(\",\")[::-1]\n",
    "            name = \" \".join(name).strip()\n",
    "            authors.append(str(name))\n",
    "   \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "def parse_solidearth_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"paperlist-object\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(\"a\", class_=\"article-title\") \n",
    "        title = title[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        authors = []\n",
    "        try:\n",
    "            author_spans = a.find_all(\"div\", class_=\"authors\")[0]\n",
    "        except IndexError:\n",
    "            print(\"hit empty authors.\")\n",
    "            continue\n",
    "    \n",
    "        for author in author_spans.string.split(\",\"):\n",
    "            author = str(author).strip()\n",
    "            if author[0:3] == \"and\":\n",
    "                author = author[3:].strip()  \n",
    "            if \" and \" in author:\n",
    "                #print(author)\n",
    "                tmp = author.split(\" and \")\n",
    "                authors.append(tmp[0])\n",
    "                authors.append(tmp[1])\n",
    "            else:\n",
    "                authors.append(author)\n",
    "\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_geophysics_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"issue-item__body\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"issue-item__title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "            \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find(\"div\", class_=\"issue-item__authors\")\n",
    "        a = author_spans.find_all(\"a\")\n",
    "        for auth in a:\n",
    "            author = str(auth.string)\n",
    "            authors.append(author)\n",
    "            \n",
    "        \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_gji_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"al-article-box\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"al-title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue        \n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"div\", class_=\"sri-authors al-authors-list\")\n",
    "        for au_span in author_spans:\n",
    "            aus = au_span.find_all(\"a\")\n",
    "            for au in aus:\n",
    "                authors.append(str(au.get_text()))\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_science_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"results-cit\")\n",
    "\n",
    "    for a in articles:\n",
    "        title_span = a.find(\"span\",  class_=\"cit-first-element\")\n",
    "        title = str(title_span.text)\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"cit-auth\")\n",
    "        for au_span in author_spans:\n",
    "            au = au_span.text.strip()\n",
    "            if au != \"and\":\n",
    "                authors.append(au)\n",
    "                \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "\n",
    "    return parsed_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create one json per article with article info, stored in parsed/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk directory where pages are saved, parse, and save parsed articles\n",
    "\n",
    "for _, _, files in os.walk(RAW_PAGES_DIR):\n",
    "    for file in files:\n",
    "        \n",
    "#         # CHANGE THIS TO FILTER FOR SPECIFIC JOURNALS\n",
    "        #if (file.startswith(\"JGR\") or file.startswith(\"GRL\") or file.startswith(\"G3\")): \n",
    "        #    continue\n",
    "               \n",
    "        parser = get_parse_function(file)\n",
    "        if parser is None:\n",
    "            print(\"got None parse function, skipping file:\", file)\n",
    "            continue\n",
    "        #print(\"processing file:\", file)\n",
    "        print(file[0:3], end=\",\")\n",
    "        journal, year, month, _ = file.split(\"_\")\n",
    "\n",
    "        html = \"\"\n",
    "        with codecs.open(RAW_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            html = infile.read()\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        for article in parser(soup, year, month, journal):\n",
    "            outfile_name = article.id[:80]+\".json\"\n",
    "            with codecs.open(PARSED_PAGES_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "                outfile.write(json.dumps(article.to_map()))\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine how many papers have initialed authors and create a dictionary with all last names from articles and abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many articles have authors with initialed names\n",
    "\n",
    "count = 0\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if not contains_initialed_name(art.names):\n",
    "            continue\n",
    "        #print(art.names)\n",
    "        count += 1\n",
    "\n",
    "        \n",
    "print(\"{count} papers from a total of {total} contain initialed author names.\".format(count = count, \n",
    "                                                                                   total = len(files)))\n",
    "\n",
    "# Create in-memory map for name guessing\n",
    "\n",
    "# This map is generated from the articles where names are not initialed.\n",
    "# The keys on the map are last names. The values are arrays of articles\n",
    "# where at least one author on the article has the keyed last name.\n",
    "\n",
    "# We create this dictionary from all articles and abstracts. \n",
    "\n",
    "count_init=0\n",
    "count_total = 0\n",
    "\n",
    "last_names_to_articles = {}\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "            \n",
    "        # considering all authors and all articles:\n",
    "        for name in art.names:\n",
    "            count_total += 1\n",
    "            # only add if it is not initialed\n",
    "            if is_initialed_name(name):\n",
    "                count_init += 1\n",
    "                continue\n",
    "            \n",
    "            last_name = name.split(\" \")[-1]            \n",
    "            \n",
    "            if last_name not in last_names_to_articles.keys():\n",
    "                last_names_to_articles[last_name] = []\n",
    "            last_names_to_articles[last_name].append(art)\n",
    "        \n",
    "print(\"All last names, map size\", len(last_names_to_articles.keys()))\n",
    "print(\"{count} author names are initialed from a total of {total} author names.\".format(count = count_init,\n",
    "                                                                                      total = count_total))\n",
    "\n",
    "\n",
    "# We repeat the same for EGU abstracts\n",
    "\n",
    "PARSED_EGU_PAGES_DIR = root+\"/egu_parsed/\"\n",
    "\n",
    "count_init=0\n",
    "count_total = 0\n",
    "\n",
    "last_names_to_abstracts = {}\n",
    "\n",
    "for _, _, files in os.walk(PARSED_EGU_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_EGU_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "            \n",
    "        # considering all authors and all articles:\n",
    "        for name in art.names:\n",
    "            count_total += 1\n",
    "            # only add if it is not initialed\n",
    "            if is_initialed_name(name):\n",
    "                count_init += 1\n",
    "                continue\n",
    "            \n",
    "            last_name = name.split(\" \")[-1]            \n",
    "            \n",
    "            if last_name not in last_names_to_abstracts.keys():\n",
    "                last_names_to_abstracts[last_name] = []\n",
    "            last_names_to_abstracts[last_name].append(art)\n",
    "        \n",
    "print(\"All last names, abstract map size\", len(last_names_to_abstracts.keys()))\n",
    "print(\"{count} author names are initialed from a total of {total} author names.\".format(count = count_init,\n",
    "                                                                                      total = count_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now try to guess their names by comparing to non-initialed authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create guesses for first author names and save to files\n",
    "def extract_name_guess(initial_name, possible_names):\n",
    "    last_name = initial_name.split(\" \")[-1]\n",
    "    for pn in possible_names:\n",
    "        if last_name != pn.split(\" \")[-1]:\n",
    "            continue  # not the name we're looking for\n",
    "        if (initial_name[0] != pn[0]) and (fuzz.token_sort_ratio(initial_name,pn) < 90):\n",
    "            continue  # first letters do not match TODO: There are cases where this is not correct, e.g.: Martin Mai and P. Martin Mai\n",
    "        if len(initial_name.split()) > 2 and len(pn.split()) > 2:\n",
    "            # both names have middle initial\n",
    "            if initial_name.split()[1][0] != pn.split()[1][0]:\n",
    "                # middle initial does not fit\n",
    "                continue\n",
    "        if len(initial_name.split(\"-\")) > 1 and len(pn.split(\"-\")) > 1:\n",
    "            # both names have hyphen\n",
    "            if initial_name.split(\"-\")[1][0] != pn.split(\"-\")[1][0]:\n",
    "                # second part of hyphenated name does not fit\n",
    "                continue\n",
    "        if is_initialed_name(pn):\n",
    "            continue\n",
    "        return pn, True\n",
    "    return \"\", False\n",
    "\n",
    "# create an output text file to quickly check if any bullshit occurs\n",
    "check_file = open(\"output_checklist_guessednames.txt\", \"w\")\n",
    "\n",
    "\n",
    "# unsupported edge cases\n",
    "# Check that the same initialized name\n",
    "# is not mapped to multiple different\n",
    "# complete names\n",
    "# J. M. Li => Jia Li\n",
    "# J. M. Li => Jiaxun Li\n",
    "# J. M. Li => Jingyuan Li\n",
    "\n",
    "count = 0\n",
    "count_guess = 0\n",
    "unmapped_names = set()\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    \n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if not contains_initialed_name(art.names): # take only article that contain initialed names\n",
    "            continue\n",
    "                \n",
    "        guessed_authors = []\n",
    "        \n",
    "        for author in art.names: # Loop through all author in the article\n",
    "            \n",
    "            if not is_initialed_name(author): # take only the authors with initialed names\n",
    "                guessed_authors.append(author) # save the full name for later writing the json file\n",
    "                continue\n",
    "                \n",
    "            last_name = author.split(\" \")[-1] # take the last name\n",
    "\n",
    "            if last_name not in last_names_to_articles.keys() and\\\n",
    "            last_name not in last_names_to_abstracts.keys(): \n",
    "                # check if the last name is in the dictionary\n",
    "                guessed_authors.append(author) # if not, we need to save it as it is\n",
    "                unmapped_names.add(author) # collect all unmapped names to double-check the performance\n",
    "                count += 1 \n",
    "                continue\n",
    "            \n",
    "            # load the articles and abstracts related to the last name\n",
    "            \n",
    "            try:\n",
    "                articles = last_names_to_articles[last_name]\n",
    "            except KeyError:\n",
    "                articles = []\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                abstracts = last_names_to_abstracts[last_name]\n",
    "            except KeyError:\n",
    "                abstracts = []\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            # gather all the guesses, paired with the size of overlap.\n",
    "            # guesses with largest overlap will be written into file. \n",
    "            \n",
    "            guesses = []\n",
    "            possible_names = set()\n",
    "            \n",
    "    \n",
    "            for article in articles + abstracts: \n",
    "            \n",
    "                overlap = article.last_name_set() & art.last_name_set() #overlap between coauthors\n",
    "\n",
    "                if (len(art.names) > 1) and (len(overlap) < 2): \n",
    "                    name, ok = extract_name_guess(author, article.names) # not always there is an overlap and sometimes... \n",
    "                    if ok:#...there is only one person in the community with this name...\n",
    "                        possible_names.add(name.split(\" \")[0] + \" \" + name.split(\" \")[-1]) #...so save the names to check this later.\n",
    "                    continue  # skip articles without enough overlap. \n",
    "            \n",
    "                name, ok = extract_name_guess(author, article.names)\n",
    "                if not ok:\n",
    "                    continue\n",
    "            \n",
    "                check_file.write(\"{}\\t\\t{}\\n\".format(name, author))\n",
    "                guessed = Guessed(art, [name], article.id)\n",
    "                guesses.append((len(overlap), guessed)) # saved the guessed name and the overlap\n",
    "            \n",
    "\n",
    "                    \n",
    "            #simplify the set 'possible_names' due to slightly different characters (e.g., accents)\n",
    "            \n",
    "            tmp = list(possible_names)\n",
    "            tmp2 = []\n",
    "\n",
    "            for i in range(len(possible_names)-1):\n",
    " \n",
    "                highest = process.extractOne(tmp[i],tmp[i+1:])\n",
    "                if highest[1] > 85:\n",
    "                    continue\n",
    "                tmp2.append(tmp[i])\n",
    "\n",
    "            if len(tmp)!=0:\n",
    "                tmp2.append(tmp[-1])\n",
    "\n",
    "            possible_names = set(tmp2)   \n",
    "\n",
    "                \n",
    "        \n",
    "            if len(guesses) == 0: #if no guesses, check if there is only one author in the field with the last name\n",
    "                if len(possible_names)==1:\n",
    "                    check_file.write(\"{}\\t\\t{}\\n\".format(list(possible_names)[0], author))\n",
    "                    guessed = Guessed(art, [list(possible_names)[0]], article.id)\n",
    "                    guesses.append((len(overlap), guessed))\n",
    "                else:\n",
    "                    guessed_authors.append(author)\n",
    "                    unmapped_names.add(author)\n",
    "                    count += 1\n",
    "                    continue\n",
    "\n",
    "            guesses = sorted(guesses, key=lambda x: x[0], reverse=True) #This does not change guesses if we do not assign it to variable guesses\n",
    "            the_guess = guesses[0][1]\n",
    "            count_guess += 1\n",
    "            guessed_authors.append(the_guess.guessed_names[0])\n",
    "            \n",
    "            \n",
    "        final_guess = Guessed(art, guessed_authors, the_guess.match_article_id)    \n",
    "        outfile_name = final_guess.id[:80]+\".json\"\n",
    "        with codecs.open(GUESSED_NAMES_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(final_guess.to_map()))\n",
    "\n",
    "check_file.close()\n",
    "print('Names not guessed:',len(unmapped_names))\n",
    "print('Number of authors not guessed:', count)\n",
    "\n",
    "print('Number of authors guessed:', count_guess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Start with the name --> gender: test genderize.io api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for gender result and test\n",
    "class GenderResult(object):\n",
    "    def __init__(self, name, result):\n",
    "        self.name = name\n",
    "        self.binary = result[0]\n",
    "        self.percent = result[1]\n",
    "        self.count = result[2]\n",
    "        \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        \n",
    "        m[\"name\"] = self.name\n",
    "        m[\"binary\"] = self.binary\n",
    "        m[\"percent\"] = self.percent\n",
    "        m[\"count\"] = self.count\n",
    "        return m\n",
    "    \n",
    "\n",
    "def gender_result_from_map(m):\n",
    "    return GenderResult(m[\"name\"], ( m[\"binary\"], m[\"percent\"], m[\"count\"]))\n",
    "\n",
    "name = \"Ismael\"\n",
    "g = getGenders(name)[0]\n",
    "gr = GenderResult(name, g)\n",
    "gender_result_from_map(json.loads(json.dumps(gr.to_map()))).to_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just read in all the previously determined and stored name --> gender pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the Gender API gender guesses into memory\n",
    "\n",
    "gender_results = []\n",
    "\n",
    "#for _, _, files in #os.walk(NAME_GENDER_DIR):\n",
    "files = glob(os.path.join(NAME_GENDER_DIR, \"*.json\"))\n",
    "for file in files:\n",
    "    with codecs.open(file, \"r\", \"utf8\") as infile:\n",
    "        gr = gender_result_from_map(json.loads(infile.read()))\n",
    "        gender_results.append(gr)\n",
    "            \n",
    "        \n",
    "    \n",
    "# Create a map of first name to gender result\n",
    "name_to_gr = {}\n",
    "for gr in gender_results:\n",
    "    name_to_gr[gr.name] = gr\n",
    "    \n",
    "print(\"Length of current name-gender map\", len(name_to_gr))\n",
    "\n",
    "# test\n",
    "print(\"Colin.\" in name_to_gr)\n",
    "print(\"First name in current gender map, gender: \", \\\n",
    "      list(name_to_gr)[0], name_to_gr[list(name_to_gr)[0]].binary)\n",
    "print(\"Last name in current gender map, gender: \", \\\n",
    "      list(name_to_gr)[-1], name_to_gr[list(name_to_gr)[-1]].binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genderize all the names\n",
    "#### - collect the list of first names from PARSED and GUESSED\n",
    "#### - check if it is in the list already, if not: call genderize io\n",
    "#### - finally check how many in the guessed initialed names are male / female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect list of all PARSED and GUESSED names to be gendered\n",
    "\n",
    "first_names = set()\n",
    "\n",
    "\n",
    "## TODO:  Adapt here for all authors instead for first author only. I think this is done now?? Can someone double-check it?\n",
    "files_guessed = glob(os.path.join(GUESSED_NAMES_DIR, \"*.json\"))\n",
    "files_parsed = glob(os.path.join(PARSED_PAGES_DIR, \"*.json\"))\n",
    "\n",
    "for file in files_guessed:\n",
    "    with codecs.open(file, \"r\", \"utf8\") as infile:\n",
    "        guess = guessed_from_map(json.loads(infile.read()))\n",
    "        \n",
    "    for name in guess.guessed_names:\n",
    "        first_name = name.split(\" \")[0]\n",
    "        if len(first_name) == 0 or is_initialed_name(name): #if it is initialed nothing to do\n",
    "            print(\"empty or initialed name?:\", name)\n",
    "            continue\n",
    "        first_names.add(first_name)\n",
    "\n",
    "\n",
    "for file in files_parsed:\n",
    "    with codecs.open(file, \"r\", \"utf8\") as infile:\n",
    "        art = article_from_map(json.loads(infile.read()))\n",
    "        \n",
    "    for name in art.names:\n",
    "        first_name = name.split(\" \")[0]\n",
    "        if len(first_name) == 0 or is_initialed_name(name):  #if it is initialed nothing to do\n",
    "            print(\"empty or initialed name?:\", name)\n",
    "            continue\n",
    "        first_names.add(first_name)\n",
    "\n",
    "print(\"number of unique first names of all authors to genderize:\", len(first_names))\n",
    "print(\"From {} to {}.\".format(list(first_names)[0],\n",
    "                              list(first_names)[-1]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Gender API and save output for all the GUESSED names\n",
    "# WARN: Makes API calls\n",
    "# Does nothing if the guessed names were genderized previously\n",
    "\n",
    "print(\"starting size:\", len(name_to_gr))\n",
    "\n",
    "nnames = 0\n",
    "to_genderize = []\n",
    "for i, name in enumerate(first_names):\n",
    "\n",
    "    if name in name_to_gr:\n",
    "        #print(\"known, \", name)\n",
    "        continue\n",
    "    else:\n",
    "        print(\"new, \", name)\n",
    "        to_genderize.append(name)\n",
    "\n",
    "    nnames += 1\n",
    "    \n",
    "    if nnames == 10: # guess the gender of 10 names simultaneously\n",
    "        result = getGenders(to_genderize)\n",
    "    #         if len(result) > 1:\n",
    "    #             print(\"long result:\", result)\n",
    "        for j, pres in enumerate(result):\n",
    "            r = pres\n",
    "            gr = GenderResult(to_genderize[j], r)\n",
    "\n",
    "            # update gender name map in memory\n",
    "            name_to_gr[gr.name] = gr\n",
    "            print(name_to_gr[gr.name].name, name_to_gr[gr.name].binary)\n",
    "\n",
    "            # also save for later use\n",
    "            file_name = \"_\".join(to_genderize[j].translate(str.maketrans('', '', string.punctuation)).split(\" \")) + \".json\"\n",
    "            \n",
    "            with codecs.open(NAME_GENDER_DIR+file_name, \"w\", \"utf8\") as outfile:\n",
    "                outfile.write(json.dumps(gr.to_map()))\n",
    "        nnames = 0\n",
    "        to_genderize = []\n",
    "\n",
    "if nnames > 0:\n",
    "    result = getGenders(to_genderize)\n",
    "    for j, pres in enumerate(result):\n",
    "        r = pres\n",
    "        gr = GenderResult(to_genderize[j], r)\n",
    "\n",
    "        # update gender name map in memory\n",
    "        name_to_gr[gr.name] = gr\n",
    "        print(name_to_gr[gr.name].name, name_to_gr[gr.name].binary)\n",
    "\n",
    "        # also save for later use\n",
    "        file_name = \"_\".join(to_genderize[j].translate(str.maketrans('', '', string.punctuation)).split(\" \")) + \".json\"\n",
    "        \n",
    "        with codecs.open(NAME_GENDER_DIR+file_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(gr.to_map()))\n",
    "    nnames = 0\n",
    "    to_genderize = []       \n",
    "    \n",
    "print(\"finish size:\", len(name_to_gr))\n",
    "\n",
    "print(name_to_gr[\"Weitao\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine how many male/female/unknown names exist in the guessed names\n",
    "# # NOTE: This ratio should be for all guesses, not the reduced set() of names\n",
    "\n",
    "# binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "\n",
    "# for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "#     for file in files:\n",
    "\n",
    "#         with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "#             guess = guessed_from_map(json.loads(infile.read()))\n",
    "#         first_name = guess.guessed_names[0].split(\" \")[0]\n",
    "#         if len(first_name) == 0:\n",
    "#             print(\"empty first name?:\", guess.guessed_names[0])\n",
    "#             continue\n",
    "#         try:\n",
    "#             gr = name_to_gr[first_name]\n",
    "#             binary_ratio_first_names[gr.binary] += 1\n",
    "#         except KeyError:\n",
    "#             print(\"not found in map:\", first_name)\n",
    "        \n",
    "        \n",
    "# binary_ratio_first_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Now, let's create a json database that can be used in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go both through the guessed names and the non-initialed names.\n",
    "# Find genders on the base of the name_to_gr map and save the results\n",
    "\n",
    "\n",
    "AUTHOR_GENDER_DIR = root + \"/author_genders/\"\n",
    "if not os.path.exists(AUTHOR_GENDER_DIR):\n",
    "    os.mkdir(AUTHOR_GENDER_DIR)\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:       \n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "            \n",
    "        if contains_initialed_name(art.names):\n",
    "            continue\n",
    "            \n",
    "        all_names = art.names\n",
    "        all_genders = []\n",
    "        all_percent = []\n",
    "\n",
    "        for n in all_names:\n",
    "            n = n.split()[0]\n",
    "            try:\n",
    "                all_genders.append(name_to_gr[n].binary)\n",
    "                all_percent.append(float(name_to_gr[n].percent))\n",
    "            except KeyError:\n",
    "                print(n)\n",
    "                #raise KeyError(\"Check name_to_gr database, it should be complete by now!\")\n",
    "        art_out = art.to_map()\n",
    "        art_out[\"all_genders\"] = all_genders\n",
    "        art_out[\"all_percent\"] = all_percent\n",
    "       \n",
    "        # save!\n",
    "        outfile_name = os.path.basename(file)\n",
    "        with codecs.open(AUTHOR_GENDER_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(art_out))\n",
    "# now do the guessed names\n",
    "for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "    for file in files:       \n",
    "        with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = guessed_from_map(json.loads(infile.read()))\n",
    "            \n",
    "        all_names = art.guessed_names\n",
    "        all_genders = []\n",
    "        all_percent = []\n",
    "\n",
    "        for n in all_names:\n",
    "            if is_initialed_name(n):\n",
    "                all_genders.append(\"init\")\n",
    "                all_percent.append(None)\n",
    "                #print(n)\n",
    "                continue\n",
    "\n",
    "            n = n.split()[0]            \n",
    "            \n",
    "            try:\n",
    "                all_genders.append(name_to_gr[n].binary)\n",
    "                all_percent.append(float(name_to_gr[n].percent))\n",
    "            except KeyError:\n",
    "                print(n)\n",
    "                #raise KeyError(\"Name {} is not gendered yet\".format(n))\n",
    "        art_out = art.to_map()\n",
    "        art_out[\"all_genders\"] = all_genders\n",
    "        art_out[\"all_percent\"] = all_percent\n",
    "        # save!\n",
    "        outfile_name = os.path.basename(file)\n",
    "        with codecs.open(AUTHOR_GENDER_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(art_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = glob(os.path.join(AUTHOR_GENDER_DIR, \"*.json\"))\n",
    "count = 0\n",
    "count_f = 0\n",
    "count_m = 0\n",
    "for file in files:\n",
    "    outfile_name = os.path.basename(file)\n",
    "    \n",
    "    #print(\"ATTENTION nmax set\")\n",
    "    with codecs.open(file, \"r\", \"utf8\") as infile:\n",
    "        art = gendered_from_map(json.loads(infile.read()))\n",
    "\n",
    "    \n",
    "    # Check if any name has gender == None\n",
    "    \n",
    "    for i, name in enumerate(art.names):\n",
    "        if art.all_genders[i] == \"init\":\n",
    "            count += 1\n",
    "            continue\n",
    "        \n",
    "        if art.all_genders[i] != \"male\" and art.all_genders[i] != \"female\":\n",
    "            count += 1\n",
    "            \n",
    "        if art.all_genders[i] == \"male\":\n",
    "            count_m += 1\n",
    "            \n",
    "        if art.all_genders[i] == \"female\":\n",
    "            count_f += 1\n",
    "            \n",
    "print('Total of uncathegorized names:', count)\n",
    "print('Total of female author names:', count_f)\n",
    "print('Total of male author names:', count_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-do guessing for those that came out as None from the namsor database (good for chinese names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API key authorization: api_key\n",
    "configuration = openapi_client.Configuration()\n",
    "configuration.api_key['X-API-KEY'] = 'd33fd630c4c66b498360a5e23a33f3b8'\n",
    "\n",
    "# create an instance of the API class\n",
    "api_instance = openapi_client.PersonalApi(openapi_client.ApiClient(configuration))\n",
    "first_name = 'Ray Y.' # str | \n",
    "last_name = 'Chuang' # str | \n",
    "\n",
    "try:\n",
    "    # Infer the likely gender of a name.\n",
    "    api_response = api_instance.gender(first_name, last_name)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling PersonalApi->gender: %s\\n\" % e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over author_gender\n",
    "AUTHOR_ALLGENDER_DIR = root + \"/author_allgenders\"\n",
    "if not os.path.exists(AUTHOR_ALLGENDER_DIR):\n",
    "    os.mkdir(AUTHOR_ALLGENDER_DIR)\n",
    "\n",
    "files = glob(os.path.join(AUTHOR_GENDER_DIR, \"*.json\"))\n",
    "nmax = 10000\n",
    "ncall = 0\n",
    "for file in files:\n",
    "    outfile_name = os.path.basename(file)\n",
    "    if os.path.exists(os.path.join(AUTHOR_ALLGENDER_DIR, outfile_name)):\n",
    "        continue\n",
    "    \n",
    "    #print(\"ATTENTION nmax set\")\n",
    "    with codecs.open(file, \"r\", \"utf8\") as infile:\n",
    "        art = gendered_from_map(json.loads(infile.read()))\n",
    "\n",
    "    \n",
    "    # Check if any name has gender == None\n",
    "    \n",
    "    for i, name in enumerate(art.names):\n",
    "        if art.all_genders[i] == \"init\":\n",
    "            continue\n",
    "        \n",
    "        if art.all_genders[i] == \"None\":\n",
    "            # print(art.names, art.all_genders, art.all_percent)\n",
    "            \n",
    "            # call namsor\n",
    "            first_name = name.split()[0] \n",
    "            last_name = name.split()[-1]\n",
    "            try:\n",
    "                # Infer the likely gender of a name.\n",
    "                api_response = api_instance.gender(first_name, last_name)\n",
    "                # pprint(api_response)\n",
    "                gender = api_response.likely_gender\n",
    "                prob = api_response.probability_calibrated\n",
    "                \n",
    "                art.all_genders[i] = gender\n",
    "                art.all_percent[i] = prob\n",
    "\n",
    "            except ApiException as e:\n",
    "                print(\"Exception when calling PersonalApi->gender: %s\\n\" % e)\n",
    "            #print(art.names, art.all_genders, art.all_percent)\n",
    "            \n",
    "            #print(\"=========================\")\n",
    "            \n",
    "            ncall += 1\n",
    "            print(ncall, end=\",\")\n",
    "        \n",
    "    if ncall > nmax:\n",
    "        break\n",
    "        \n",
    "    # overwrite the file\n",
    "    art_out = art.to_map()\n",
    "\n",
    "    # save!\n",
    "    with codecs.open(os.path.join(AUTHOR_ALLGENDER_DIR, outfile_name), \"w\", \"utf8\") as outfile:\n",
    "        outfile.write(json.dumps(art_out))        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHOR_ALLGENDER_DIR = root + \"/author_allgenders\"\n",
    "\n",
    "files = glob(os.path.join(AUTHOR_ALLGENDER_DIR, \"*.json\"))\n",
    "count = 0\n",
    "count_f = 0\n",
    "count_m = 0\n",
    "for file in files:\n",
    "    outfile_name = os.path.basename(file)\n",
    "    \n",
    "    #print(\"ATTENTION nmax set\")\n",
    "    with codecs.open(file, \"r\", \"utf8\") as infile:\n",
    "        art = gendered_from_map(json.loads(infile.read()))\n",
    "\n",
    "    \n",
    "    # Check if any name has gender == None\n",
    "    \n",
    "    for i, name in enumerate(art.names):\n",
    "        if art.all_genders[i] == \"init\":\n",
    "            count += 1\n",
    "            continue\n",
    "        \n",
    "        if art.all_genders[i] != \"male\" and art.all_genders[i] != \"female\":\n",
    "            count += 1\n",
    "            \n",
    "        if art.all_genders[i] == \"male\":\n",
    "            print(name,art.all_genders[i],art.all_percent[i])\n",
    "            count_m += 1\n",
    "            \n",
    "        if art.all_genders[i] == \"female\":\n",
    "            print(name,art.all_genders[i],art.all_percent[i])\n",
    "            count_f += 1\n",
    "            \n",
    "print('Total of uncathegorized names:', count)\n",
    "print('Total of female author names:', count_f)\n",
    "print('Total of male author names:', count_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following part may be moved to a new notebook. It is from the analysis of Pico et al. 2020-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist in the guessed names\n",
    "# printed out for each journal.\n",
    "\n",
    "\n",
    "journal_names = [\n",
    "\"Tectonophysics\",\n",
    "\"Physics+of+the+Earth+and+Planetary+Interiors\",\n",
    "\"Earth+and+Planetary+Science+Letters\",\n",
    "\"SolidEarth\",\n",
    "\"GEOPHYSICS\",\n",
    "\"NatureGeoscience\",\n",
    "\"GRL\",\n",
    "\"JGRSolidEarth\",\n",
    "\"G3\",\n",
    "\"GJI\",\n",
    "\"Nature\",\n",
    "\"Bulletin+of+the+Seismological+Society+of+America\",\n",
    "\"Seismological+Research+Letters\"]\n",
    "\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "    \n",
    "    for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                guess = guessed_from_map(json.loads(infile.read()))\n",
    "            first_name = guess.guessed_name.split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", guess.guessed_name)\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                binary_ratio_first_names[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "    print(journal, binary_ratio_first_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist for the un-initialed names\n",
    "# printed out for each journal.\n",
    "\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "    \n",
    "    initialed_name_count = 0\n",
    "    \n",
    "    for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                article = article_from_map(json.loads(infile.read()))\n",
    "            first_name = article.first_author.split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", article.first_author)\n",
    "                continue\n",
    "            if is_initialed_name(first_name):\n",
    "                initialed_name_count += 1\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                binary_ratio_first_names[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "    print(journal, binary_ratio_first_names, \"initialed name count:\", initialed_name_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but split into year/month bins\n",
    "#\n",
    "# Determine how many male/female/unknown names exist for the un-initialed names\n",
    "# printed out for each journal.\n",
    "\n",
    "journal_tallys = {}\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0, 'initialed':0}\n",
    "    \n",
    "    initialed_name_count = 0\n",
    "    \n",
    "    for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                article = article_from_map(json.loads(infile.read()))\n",
    "                \n",
    "            tally = journal_tallys.setdefault(journal, {}).setdefault(article.year, {}).setdefault(article.month, {'male':0, 'female':0, 'None':0, 'initialed':0})\n",
    "            \n",
    "            first_name = article.first_author.strip().split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", article.first_author)\n",
    "                continue\n",
    "            if is_initialed_name(first_name):\n",
    "                tally['initialed'] += 1\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                tally[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "for journal, values in journal_tallys.items():\n",
    "    print(journal, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
