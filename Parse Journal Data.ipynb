{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "import gender\n",
    "from gender import getGenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure local paths\n",
    "\n",
    "root = ! pwd\n",
    "root = root[0]\n",
    "\n",
    "print(\"using root directory:\", root)\n",
    "\n",
    "RAW_PAGES_DIR=root+\"/pages/\"\n",
    "PARSED_PAGES_DIR=root+\"/parsed/\"\n",
    "GUESSED_NAMES_DIR=root+\"/guessed/\"\n",
    "NAME_GENDER_DIR=root+\"/name_genders/\"\n",
    "\n",
    "# create directories if they do not exist\n",
    "\n",
    "for d in [RAW_PAGES_DIR,\n",
    "         PARSED_PAGES_DIR,\n",
    "         GUESSED_NAMES_DIR,\n",
    "         NAME_GENDER_DIR]:\n",
    "    try:\n",
    "        os.mkdir(d)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article class, Guessed class, and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and define useful functions and\n",
    "# data structures.\n",
    "\n",
    "def is_initialed_name(name):\n",
    "    first_term = name.split(\" \")[0]\n",
    "    if len(first_term) == 0:\n",
    "        # print(\"first term len zero. name:\", name)\n",
    "        return False\n",
    "    return first_term[-1] == \".\" and first_term[:-1].isupper()\n",
    "\n",
    "print(\"test is_initialed_name- True:\", is_initialed_name(\"J. Smith\"), \", False:\", is_initialed_name(\"Joe Smith\"))\n",
    "\n",
    "def contains_initialed_name(names):\n",
    "    for name in names:\n",
    "        if is_initialed_name(name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"test contains_initialed_name- True\", contains_initialed_name([\"J. Smith\", \"Cat Meowins\"]), \", False:\", contains_initialed_name([\"Joe Smith\", \"Cat Meowins\"]))\n",
    "\n",
    "def clean_name(name):\n",
    "    name = name.strip()\n",
    "    name = re.sub(r'\\s', ' ', name)\n",
    "    if not is_initialed_name(name):\n",
    "        terms = name.split(\" \")\n",
    "        terms[0] = terms[0].strip(\".\")\n",
    "        name = \" \".join(terms)\n",
    "    return name.strip('.')\n",
    "\n",
    "print(\"test clean name- Colin. J. Cats:\", clean_name(\"Colin. J. Cats\"), \"W. B. Easy:\", clean_name(\"W. B. Easy\"))\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, first_author, all_names, year, month, title, journal):\n",
    "        # clean the author names\n",
    "        # - remove non-ascii whitespace\n",
    "        # - strip bookend whitespace\n",
    "        # - strip periods from first names if not an initialed name\n",
    "        \n",
    "        \n",
    "        self.first_author = clean_name(first_author)\n",
    "        self.names = [clean_name(name) for name in all_names]\n",
    "        self.year = year\n",
    "        self.month = month\n",
    "        self.title = title\n",
    "        self.journal = journal\n",
    "        \n",
    "        # create a unique identifier for this article\n",
    "        self.id = \"_\".join([journal, year, month, \"_\".join(title.translate(str.maketrans('', '', string.punctuation)).split(\" \"))])\n",
    "        \n",
    "        # determine if article has initialed names\n",
    "        self.has_initials = is_initialed_name(first_author)\n",
    "    \n",
    "    def last_name_set(self):\n",
    "        # return a set() of all the last names\n",
    "        name_set = set()\n",
    "        for name in self.names:\n",
    "            name_set.add(name.split(\" \")[-1])\n",
    "        return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"has_initials\"] = self.has_initials\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def article_from_map(article_map):\n",
    "    return Article(\n",
    "        first_author=article_map[\"first_author\"],\n",
    "        all_names=article_map[\"all_names\"],\n",
    "        year=article_map[\"year\"],\n",
    "        month=article_map[\"month\"],\n",
    "        title=article_map[\"title\"],\n",
    "        journal=article_map[\"journal\"])\n",
    "\n",
    "test_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"A. Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2019\",\n",
    "    month=\"02\",\n",
    "    title=\"a story of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "\n",
    "print(\"last name set:\", test_article.last_name_set())\n",
    "print(\"article id:\", test_article.id)\n",
    "print(\"article has initial:\", test_article.has_initials)\n",
    "print(\"article map:\", test_article.to_map())\n",
    "\n",
    "\n",
    "class Guessed:\n",
    "    def __init__(self, primary_article, guessed_name, match_article_id):\n",
    "        self.first_author = primary_article.first_author\n",
    "        self.names = primary_article.names\n",
    "        self.year = primary_article.year\n",
    "        self.month = primary_article.month\n",
    "        self.title = primary_article.title\n",
    "        self.journal = primary_article.journal\n",
    "        \n",
    "        # create a unique identifier for this article\n",
    "        self.id = \"_\".join([self.journal, self.year, self.month, \"_\".join(self.title.translate(str.maketrans('', '', string.punctuation)).split(\" \"))])\n",
    "        \n",
    "        # determine if article has initialed names\n",
    "        self.has_initials = is_initialed_name(self.first_author)\n",
    "        \n",
    "        self.guessed_name = guessed_name\n",
    "        self.match_article_id = match_article_id\n",
    "    \n",
    "    def last_name_set(self):\n",
    "        # return a set() of all the last names\n",
    "        name_set = set()\n",
    "        for name in self.names:\n",
    "            name_set.add(name.split(\" \")[-1])\n",
    "        return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"has_initials\"] = self.has_initials\n",
    "        m[\"guessed_name\"] = self.guessed_name\n",
    "        m[\"match_article_id\"] = self.match_article_id\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def guessed_from_map(guessed_map):\n",
    "    g = Guessed(\n",
    "        primary_article=article_from_map(guessed_map),\n",
    "        guessed_name=guessed_map[\"guessed_name\"],\n",
    "        match_article_id=guessed_map[\"match_article_id\"])\n",
    "    return g\n",
    "\n",
    "test_primary_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"A. Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2019\",\n",
    "    month=\"02\",\n",
    "    title=\"a story of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "test_match_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"Arthur Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2018\",\n",
    "    month=\"05\",\n",
    "    title=\"existence of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "test_guessed = Guessed(test_primary_article, \"Arthur Cat\", test_match_article.id)\n",
    "\n",
    "print(test_guessed.to_map())\n",
    "print(guessed_from_map(test_guessed.to_map()).to_map() == test_guessed.to_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page parse functions\n",
    "\n",
    "def get_parse_function(filename):\n",
    "    if filename.startswith(\"JGR\") or filename.startswith(\"GRL\") or filename.startswith(\"G3\"):\n",
    "        return parse_agu_page\n",
    "    if filename.startswith(\"Seismological+Research+Letters\") or filename.startswith(\"Bulletin+of+the+Seismological+Society+of+America\"):\n",
    "        return parse_gsw_page\n",
    "    if filename.startswith(\"NatureGeoscience\") or filename.startswith(\"Nature\"):\n",
    "        return parse_ng_page\n",
    "    if filename.startswith(\"Earth+and+Planetary+Science+Letters\") or filename.startswith(\"Physics+of+the+Earth+and+Planetary+Interiors\") or filename.startswith(\"Tectonophysics\"):\n",
    "        return parse_sd_page\n",
    "    if filename.startswith(\"GJI\"):\n",
    "        return parse_gji_page\n",
    "    if filename.startswith(\"SolidEarth\"):\n",
    "        return parse_solidearth_page\n",
    "    if filename.startswith(\"GEOPHYSICS\"):\n",
    "        return parse_geophysics_page\n",
    "    if filename.startswith(\"Science\"):\n",
    "        return parse_science_page\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_agu_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"item__body\")\n",
    "    for a in articles:\n",
    "        meta_title = a.find_all(class_=\"meta__title\")\n",
    "        title = meta_title[0].find_all(\"a\", class_=\"publication_title\")\n",
    "        title = title[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "       \n",
    "        if title == \"Issue Information\":\n",
    "            print('hit \"Issue Information\"')\n",
    "            continue\n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "\n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"a\", class_=\"publication_contrib_author\")\n",
    "        for p in author_spans:\n",
    "            author_span = p.span\n",
    "            if author_span.i is not None:\n",
    "                author_span.i.decompose()\n",
    "            authors.append(str(author_span.string))\n",
    "   \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "\n",
    "        article = Article(\n",
    "                first_author=authors[0],\n",
    "                all_names=authors,\n",
    "                year=year,\n",
    "                month=month,\n",
    "                title=title,\n",
    "                journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "\n",
    "def parse_gsw_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"al-article-box\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"al-title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"wi-fullname\")\n",
    "        for author in author_spans:\n",
    "            author = str(author.find_all(\"a\")[0].get_text())\n",
    "            authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_ng_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"li\", class_=\"pb20\")\n",
    "\n",
    "    for a in articles:\n",
    "        title_section = a.find_all(\"h2\", class_=\"h3\")\n",
    "        if title_section[0] is None:\n",
    "            print(\"title section is none:\", a)\n",
    "            continue\n",
    "        title = title_section[0].a.contents[0]\n",
    "        \n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "        title = title.strip()\n",
    "        \n",
    "        \n",
    "        authors = []\n",
    "        author_span = a.find_all(\"ul\", class_=\"js-list-authors-3\")\n",
    "        for auths in author_span:\n",
    "            for author in auths.find_all(\"a\", class_=\"js-no-scroll\"):\n",
    "                author = author.contents[0]\n",
    "                \n",
    "                if author == \"[…]\" or \"Show fewer authors\" in author:\n",
    "                    continue\n",
    "                    \n",
    "                authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "        \n",
    "    return parsed_articles\n",
    "\n",
    "        \n",
    "def parse_sd_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"result-item-container\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(\"a\", class_=\"result-list-title-link\")\n",
    "        title = title[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "\n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            print(\"title:\", a.find_all(\"a\", class_=\"result-list-title-link\"))\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title\")\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"author\")\n",
    "        for author in author_spans:\n",
    "            author = author.string\n",
    "            authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_solidearth_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"paperlist-object\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(\"a\", class_=\"article-title\") \n",
    "        title = title[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        authors = []\n",
    "        try:\n",
    "            author_spans = a.find_all(\"div\", class_=\"authors\")[0]\n",
    "        except IndexError:\n",
    "            print(\"hit empty authors.\")\n",
    "            continue\n",
    "    \n",
    "        for author in author_spans.string.split(\",\"):\n",
    "            author = str(author).strip()\n",
    "            if author[0:3] == \"and\":\n",
    "                author = author[3:].strip()            \n",
    "            authors.append(author)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_geophysics_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"issue-item__body\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"issue-item__title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "            \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find(\"div\", class_=\"issue-item__authors\")\n",
    "        a = author_spans.find_all(\"a\")\n",
    "        for auth in a:\n",
    "            author = str(auth.string)\n",
    "            authors.append(author)\n",
    "        \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_gji_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"al-article-box\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"al-title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue        \n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"div\", class_=\"sri-authors al-authors-list\")\n",
    "        for au_span in author_spans:\n",
    "            aus = au_span.find_all(\"a\")\n",
    "            for au in aus:\n",
    "                authors.append(str(au.get_text()))\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_science_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"results-cit\")\n",
    "\n",
    "    for a in articles:\n",
    "        title_span = a.find(\"span\",  class_=\"cit-first-element\")\n",
    "        title = str(title_span.text)\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"cit-auth\")\n",
    "        for au_span in author_spans:\n",
    "            au = au_span.text.strip()\n",
    "            if au != \"and\":\n",
    "                authors.append(au)\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "\n",
    "    return parsed_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create one json per article with article info, stored in parsed/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk directory, parse, and save parsed articles\n",
    "\n",
    "for _, _, files in os.walk(RAW_PAGES_DIR):\n",
    "    for file in files:\n",
    "        \n",
    "#         # CHANGE THIS TO FILTER FOR SPECIFIC JOURNALS\n",
    "        #if (file.startswith(\"JGR\") or file.startswith(\"GRL\") or file.startswith(\"G3\")): \n",
    "        #    continue\n",
    "        \n",
    "        parser = get_parse_function(file)\n",
    "        if parser is None:\n",
    "            print(\"got None parse function, skipping file:\", file)\n",
    "            continue\n",
    "        #print(\"processing file:\", file)\n",
    "        print(file[0:3], end=\",\")\n",
    "        journal, year, month, _ = file.split(\"_\")\n",
    "\n",
    "        html = \"\"\n",
    "        with codecs.open(RAW_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            html = infile.read()\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        for article in parser(soup, year, month, journal):\n",
    "            outfile_name = article.id[:80]+\".json\"\n",
    "            with codecs.open(PARSED_PAGES_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "                outfile.write(json.dumps(article.to_map()))\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with the name --> gender: test genderize.io api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for gender result and test\n",
    "class GenderResult(object):\n",
    "    def __init__(self, name, result):\n",
    "        self.name = name\n",
    "        self.binary = result[0]\n",
    "        self.percent = result[1]\n",
    "        self.count = result[2]\n",
    "        \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"name\"] = self.name\n",
    "        m[\"binary\"] = self.binary\n",
    "        m[\"percent\"] = self.percent\n",
    "        m[\"count\"] = self.count\n",
    "        return m\n",
    "    \n",
    "\n",
    "def gender_result_from_map(m):\n",
    "    return GenderResult(m[\"name\"], ( m[\"binary\"], m[\"percent\"], m[\"count\"]))\n",
    "\n",
    "name = \"Taylor\"\n",
    "g = getGenders(name)[0]\n",
    "gr = GenderResult(name, g)\n",
    "gender_result_from_map(json.loads(json.dumps(gr.to_map()))).to_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read all the previously determined and stored name --> gender pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the Gender API gender guesses into memory\n",
    "\n",
    "gender_results = []\n",
    "\n",
    "for _, _, files in os.walk(NAME_GENDER_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(NAME_GENDER_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            gr = gender_result_from_map(json.loads(infile.read()))\n",
    "            gender_results.append(gr)\n",
    "            \n",
    "print(\"We have *{}* stored name informations in\\n{}\\nfrom previous runs.\".format(len(gender_results), NAME_GENDER_DIR))\n",
    "        \n",
    "    \n",
    "# Create a map of first name to gender result\n",
    "name_to_gr = {}\n",
    "for gr in gender_results:\n",
    "    name_to_gr[gr.name] = gr\n",
    "    \n",
    "print(\"Length of current name-gender map\", len(name_to_gr))\n",
    "\n",
    "# test\n",
    "print(\"Colin.\" in name_to_gr)\n",
    "print(\"First name in current gender map, gender: \", \\\n",
    "      list(name_to_gr)[0], name_to_gr[list(name_to_gr)[0]].binary)\n",
    "print(\"Last name in current gender map, gender: \", \\\n",
    "      list(name_to_gr)[-1], name_to_gr[list(name_to_gr)[-1]].binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine how many papers have initialed authors, and try to guess their names by comparing to non-initialed authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many documents have initialed first authors\n",
    "count = 0\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if not art.has_initials:\n",
    "            continue\n",
    "        count += 1\n",
    "        \n",
    "print(\"{} papers have initialed author names.\".format(count))\n",
    "\n",
    "# Create in-memory map for name guessing\n",
    "\n",
    "# This map is generated from the articles where names are not initialed.\n",
    "# The keys on the map are last names. The values are arrays of articles\n",
    "# where at least one author on the article has the keyed last name.\n",
    "\n",
    "\n",
    "## TODO: This could be improved. last_names_to_articles dictionary should also\n",
    "# contain the last names of the coauthors.\n",
    "\n",
    "last_names_to_articles = {}\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if art.has_initials:\n",
    "            continue\n",
    "        #last_name = art.first_author.split(\" \")[-1]\n",
    "        #if last_name not in last_names_to_articles.keys():\n",
    "        #    last_names_to_articles[last_name] = []\n",
    "        #last_names_to_articles[last_name].append(art)\n",
    "        for name in art.names:\n",
    "            last_name = name.split(\" \")[-1]\n",
    "            if last_name not in last_names_to_articles.keys():\n",
    "                last_names_to_articles[last_name] = []\n",
    "            last_names_to_articles[last_name].append(art)\n",
    "        \n",
    "print(\"All last names, map size\", len(last_names_to_articles.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create guesses for first author names and save to files\n",
    "def extract_name_guess(initial_name, possible_names):\n",
    "    last_name = initial_name.split(\" \")[-1]\n",
    "    for pn in possible_names:\n",
    "        if last_name != pn.split(\" \")[-1]:\n",
    "            continue  # not the name we're looking for\n",
    "        if initial_name[0] != pn[0]:\n",
    "            continue  # first letters do not match\n",
    "        if len(initial_name.split()) > 2 and len(pn.split()) > 2:\n",
    "            # both names have middle initial\n",
    "            if initial_name.split()[1] != pn.split()[1]:\n",
    "                # middle initial does not fit\n",
    "                continue\n",
    "        if len(initial_name.split(\"-\")) > 1 and len(pn.split(\"-\")) > 1:\n",
    "            # both names have hyphen\n",
    "            if initial_name.split(\"-\")[1][0] != pn.split(\"-\")[1][0]:\n",
    "                # second part of hyphenated name does not fit\n",
    "                continue\n",
    "        if is_initialed_name(pn):\n",
    "            continue\n",
    "        return pn, True\n",
    "    return \"\", False\n",
    "\n",
    "# create an output text file to quickly check if any bullshit occurs\n",
    "check_file = open(\"output_checklist_guessednames.txt\", \"w\")\n",
    "\n",
    "# create a list that keeps track of guesses to flag ambiguity\n",
    "all_guesses = {}\n",
    "\n",
    "# unsupported edge cases\n",
    "# Check that the same initialized name\n",
    "# is not mapped to multiple different\n",
    "# complete names\n",
    "# J. M. Li => Jia Li\n",
    "# J. M. Li => Jiaxun Li\n",
    "# J. M. Li => Jingyuan Li\n",
    "\n",
    "\n",
    "unmapped_names = set()\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if not art.has_initials:\n",
    "            continue\n",
    "        last_name = art.first_author.split(\" \")[-1]\n",
    "        if last_name not in last_names_to_articles.keys():\n",
    "            unmapped_names.add(art.first_author)\n",
    "            continue\n",
    "        articles = last_names_to_articles[last_name]\n",
    "        \n",
    "        # gather all the guesses, paired with the size of overlap.\n",
    "        # guesses with largest overlap will be written to file. \n",
    "        guesses = []\n",
    "        for article in articles:\n",
    "            overlap = article.last_name_set() & art.last_name_set()\n",
    "            if len(overlap) < 2:\n",
    "                continue  # skip articles without enough overlap\n",
    "            \n",
    "            name, ok = extract_name_guess(art.first_author, article.names)\n",
    "            if not ok:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                previous_guess = all_guesses[art.first_author]\n",
    "                if previous_guess != name:\n",
    "                    print(\"Detected ambiguous guess: {} - {} - {}\".format(\n",
    "                         art.first_author, previous_guess, name))\n",
    "                    print(art.title)\n",
    "                    # we could continue at this point, but we already have one of these names\n",
    "                    # written in a json (previous_guess). Not sure what to do, let's discuss\n",
    "            except KeyError:\n",
    "                pass\n",
    "            check_file.write(\"{}\\t\\t{}\\n\".format(name, art.first_author))\n",
    "            guessed = Guessed(art, name, article.id)\n",
    "            guesses.append((len(overlap), guessed))\n",
    "            all_guesses[art.first_author] = name\n",
    "            \n",
    "        if len(guesses) == 0:\n",
    "            continue\n",
    "        \n",
    "        guesses = sorted(guesses, key=lambda x: x[0], reverse=True) #This does not change guesses if we do not assign it to variable guesses\n",
    "        the_guess = guesses[0][1]\n",
    "             \n",
    "        outfile_name = the_guess.id[:80]+\".json\"\n",
    "        with codecs.open(GUESSED_NAMES_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(the_guess.to_map()))\n",
    "check_file.close()\n",
    "len(unmapped_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Genderize the names GUESSED from initials\n",
    "#### - collect the list of first names\n",
    "#### - check if it is in the list already, if not: call genderize io\n",
    "#### - finally check how many in the guessed initialed names are male / female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect list of all GUESSED names to be gendered\n",
    "\n",
    "guessed_first_names = set()\n",
    "\n",
    "for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            guess = guessed_from_map(json.loads(infile.read()))\n",
    "        first_name = guess.guessed_name.split(\" \")[0]\n",
    "        if len(first_name) == 0:\n",
    "            print(\"empty first name?:\", guess.guessed_name)\n",
    "            continue\n",
    "        guessed_first_names.add(first_name)\n",
    "        \n",
    "print(\"number of unique first names to guess:\", len(guessed_first_names))\n",
    "print(\"From {} to {}.\".format(list(guessed_first_names)[0],\n",
    "                              list(guessed_first_names)[-1]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Gender API and save output for all the GUESSED names\n",
    "# WARN: Makes API calls\n",
    "# Does nothing if the guessed names were genderized previously\n",
    "for i, name in enumerate(guessed_first_names):\n",
    "    # print(i, end=\",\")\n",
    "    if name in name_to_gr:\n",
    "        continue\n",
    "    else:\n",
    "        result = getGenders(name)\n",
    "        if len(result) > 1:\n",
    "            print(\"long result:\", result)\n",
    "        r = result[0]\n",
    "        gr = GenderResult(name, r)\n",
    "        \n",
    "        # update gender name map in memory\n",
    "        name_to_gr[gr.name] = gr\n",
    "        \n",
    "        # also save for later use\n",
    "        file_name = \"_\".join(name.translate(str.maketrans('', '', string.punctuation)).split(\" \")) + \".json\"\n",
    "        with codecs.open(NAME_GENDER_DIR+file_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(gr.to_map()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist in the guessed names\n",
    "# NOTE: This ratio should be for all guesses, not the reduced set() of names\n",
    "\n",
    "binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "\n",
    "for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "    for file in files:\n",
    "#         if not file.startswith(\"JGROceans\"):\n",
    "#             continue\n",
    "        with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            guess = guessed_from_map(json.loads(infile.read()))\n",
    "        first_name = guess.guessed_name.split(\" \")[0]\n",
    "        if len(first_name) == 0:\n",
    "            print(\"empty first name?:\", guess.guessed_name)\n",
    "            continue\n",
    "        try:\n",
    "            gr = name_to_gr[first_name]\n",
    "            binary_ratio_first_names[gr.binary] += 1\n",
    "        except KeyError:\n",
    "            print(\"not found in map:\", first_name)\n",
    "        \n",
    "        \n",
    "binary_ratio_first_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now genderize the names that were given in full\n",
    "#### - collect the list of first names\n",
    "#### - check if name is in the list already, if not: call genderize io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of first names from all scraped journals\n",
    "# For Non-Initialed names\n",
    "\n",
    "all_first_names = set()\n",
    "all_names = set()\n",
    "\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if art.has_initials:\n",
    "            continue\n",
    "        first_name = art.first_author.split(\" \")[0]\n",
    "        all_first_names.add(first_name)\n",
    "        \n",
    "        for allname in art.names:\n",
    "            all_names.add(allname.split()[0])\n",
    "        \n",
    "        \n",
    "print(\"first name set size\", len(all_first_names))\n",
    "print(\"all author name set size\", len(all_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the gender API for all the non-initialed first names. Deduped against\n",
    "# existing files in the map name_to_gr. Be sure to run the cell\n",
    "# that loads that map before running this function. When properly\n",
    "# loaded, this cell can be re-run until all names are handled.\n",
    "\n",
    "# TODO : Also run this on ALL author names!\n",
    "\n",
    "print(\"starting size:\", len(name_to_gr))\n",
    "\n",
    "handled = 0\n",
    "\n",
    "for name in all_first_names:\n",
    "    if name in name_to_gr:\n",
    "        continue\n",
    "        \n",
    "    handled += 1\n",
    "    \n",
    "    result = getGenders(name)\n",
    "    if len(result) > 1:\n",
    "        print(\"long result:\", result)\n",
    "    r = result[0]\n",
    "    gr = GenderResult(name, r)\n",
    "    \n",
    "    # update map currently in memory\n",
    "    name_to_gr[name] = gr\n",
    "    \n",
    "    # and save for later use\n",
    "    file_name = \"_\".join(name.translate(str.maketrans('', '', string.punctuation)).split(\" \")) + \".json\"\n",
    "    with codecs.open(NAME_GENDER_DIR+file_name, \"w\", \"utf8\") as outfile:\n",
    "        outfile.write(json.dumps(gr.to_map()))\n",
    "        \n",
    "    if handled > 970:\n",
    "        print(\"hit request limit\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "print(\"finish size:\", len(name_to_gr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, evaluate for all journals / each journal / journal per year..\n",
    "#### Here, let's create a json database that can be used in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go both through the guessed names and the non-initialed names.\n",
    "# Find genders on the base of the name_to_gr map and save the results\n",
    "\n",
    "# While we do not yet have a name database large enough to go through all the names,\n",
    "# just go through a few for testing purposes:\n",
    "nfile = 100\n",
    "\n",
    "AUTHOR_GENDER_DIR = root + \"/author_genders/\"\n",
    "if not os.path.exists(AUTHOR_GENDER_DIR):\n",
    "    os.mkdir(AUTHOR_GENDER_DIR)\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files[0:nfile]:       \n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if art.has_initials:\n",
    "            continue\n",
    "            \n",
    "        all_names = art.names\n",
    "        all_genders = []\n",
    "        all_percent = []\n",
    "\n",
    "        for n in all_names:\n",
    "            n = n.split()[0]\n",
    "            try:\n",
    "                all_genders.append(name_to_gr[n].binary)\n",
    "                all_percent.append(name_to_gr[n].percent)\n",
    "            except KeyError:\n",
    "                print(\"Name {} is not gendered yet\".format(n))\n",
    "                all_genders.append(None)\n",
    "                all_percent.append(None)\n",
    "        art_out = art.to_map()\n",
    "        art_out[\"all_genders\"] = all_genders\n",
    "        art_out[\"all_percent\"] = all_percent\n",
    "        #print(art_out[\"all_names\"])\n",
    "        print(art_out[\"all_genders\"])\n",
    "       \n",
    "        # save!\n",
    "        outfile_name = os.path.basename(file)\n",
    "        with codecs.open(AUTHOR_GENDER_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(art_out))\n",
    "# now do the guessed names\n",
    "for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "    for file in files[0: nfile]:       \n",
    "        with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if art.has_initials:\n",
    "            continue\n",
    "            \n",
    "        all_names = art.names\n",
    "        all_genders = []\n",
    "        all_percent = []\n",
    "\n",
    "        for n in all_names:\n",
    "            n = n.split()[0]\n",
    "            try:\n",
    "                all_genders.append(name_to_gr[n].binary)\n",
    "                all_percent.append(name_to_gr[n].percent)\n",
    "            except KeyError:\n",
    "                print(\"Name {} is not gendered yet\".format(n))\n",
    "                all_genders.append(None)\n",
    "                all_percent.append(None)\n",
    "        art_out = art.to_map()\n",
    "        art_out[\"all_genders\"] = all_genders\n",
    "        art_out[\"all_percent\"] = all_percent\n",
    "        print(art_out[\"all_genders\"])\n",
    "        # save!\n",
    "        outfile_name = os.path.basename(file)\n",
    "        with codecs.open(AUTHOR_GENDER_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(art_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following part may be moved to a new notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist in the guessed names\n",
    "# printed out for each journal.\n",
    "\n",
    "\n",
    "journal_names = [\n",
    "\"Tectonophysics\",\n",
    "\"Physics+of+the+Earth+and+Planetary+Interiors\",\n",
    "\"Earth+and+Planetary+Science+Letters\",\n",
    "\"SolidEarth\",\n",
    "\"GEOPHYSICS\",\n",
    "\"NatureGeoscience\",\n",
    "\"GRL\",\n",
    "\"JGRSolidEarth\",\n",
    "\"G3\",\n",
    "\"GJI\",\n",
    "\"Nature\",\n",
    "\"Bulletin+of+the+Seismological+Society+of+America\",\n",
    "\"Seismological+Research+Letters\"]\n",
    "\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "    \n",
    "    for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                guess = guessed_from_map(json.loads(infile.read()))\n",
    "            first_name = guess.guessed_name.split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", guess.guessed_name)\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                binary_ratio_first_names[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "    print(journal, binary_ratio_first_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist for the un-initialed names\n",
    "# printed out for each journal.\n",
    "\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "    \n",
    "    initialed_name_count = 0\n",
    "    \n",
    "    for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                article = article_from_map(json.loads(infile.read()))\n",
    "            first_name = article.first_author.split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", article.first_author)\n",
    "                continue\n",
    "            if is_initialed_name(first_name):\n",
    "                initialed_name_count += 1\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                binary_ratio_first_names[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "    print(journal, binary_ratio_first_names, \"initialed name count:\", initialed_name_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but split into year/month bins\n",
    "#\n",
    "# Determine how many male/female/unknown names exist for the un-initialed names\n",
    "# printed out for each journal.\n",
    "\n",
    "journal_tallys = {}\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0, 'initialed':0}\n",
    "    \n",
    "    initialed_name_count = 0\n",
    "    \n",
    "    for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                article = article_from_map(json.loads(infile.read()))\n",
    "                \n",
    "            tally = journal_tallys.setdefault(journal, {}).setdefault(article.year, {}).setdefault(article.month, {'male':0, 'female':0, 'None':0, 'initialed':0})\n",
    "            \n",
    "            first_name = article.first_author.strip().split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", article.first_author)\n",
    "                continue\n",
    "            if is_initialed_name(first_name):\n",
    "                tally['initialed'] += 1\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                tally[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "for journal, values in journal_tallys.items():\n",
    "    print(journal, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
