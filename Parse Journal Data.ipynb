{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "import gender\n",
    "from gender import getGenders\n",
    "\n",
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure local paths\n",
    "\n",
    "root = ! pwd\n",
    "root = root[0]\n",
    "\n",
    "print(\"using root directory:\", root)\n",
    "\n",
    "RAW_PAGES_DIR=root+\"/pages/\"\n",
    "PARSED_PAGES_DIR=root+\"/parsed/\"\n",
    "GUESSED_NAMES_DIR=root+\"/guessed/\"\n",
    "NAME_GENDER_DIR=root+\"/name_genders/\"\n",
    "\n",
    "# create directories if they do not exist\n",
    "\n",
    "for d in [RAW_PAGES_DIR,\n",
    "         PARSED_PAGES_DIR,\n",
    "         GUESSED_NAMES_DIR,\n",
    "         NAME_GENDER_DIR]:\n",
    "    try:\n",
    "        os.mkdir(d)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rm $GUESSED_NAMES_DIR*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article class, Guessed class, and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and define useful functions and\n",
    "# data structures.\n",
    "\n",
    "def is_initialed_name(name):\n",
    "    first_term = name.split(\" \")[0]\n",
    "    if len(first_term) == 0:\n",
    "        # print(\"first term len zero. name:\", name)\n",
    "        return False\n",
    "    return first_term[-1] == \".\" and first_term[:-1].isupper() or len(first_term) == 1 or len(first_term.split(\"-\")[0]) == 1\n",
    "\n",
    "\n",
    "print(\"test is_initialed_name- True:\", is_initialed_name(\"J. Smith\"), \", False:\", is_initialed_name(\"Joe Smith\"),\n",
    "     \", True:\", is_initialed_name(\"J Smith\"), \", True:\", is_initialed_name(\"J-P Ampuero\"))\n",
    "\n",
    "def contains_initialed_name(names):\n",
    "    for name in names:\n",
    "        if is_initialed_name(name):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"test contains_initialed_name- True\", contains_initialed_name([\"J. Smith\", \"Cat Meowins\"]), \", False:\", contains_initialed_name([\"Joe Smith\", \"Cat Meowins\"]))\n",
    "\n",
    "def clean_name(name):\n",
    "    name = name.strip()\n",
    "    name = re.sub(r'\\s', ' ', name)\n",
    "    if not is_initialed_name(name):\n",
    "        terms = name.split(\" \")\n",
    "        terms[0] = terms[0].strip(\".\")\n",
    "        name = \" \".join(terms)\n",
    "    return name.strip('.')\n",
    "\n",
    "print(\"test clean name- Colin. J. Cats:\", clean_name(\"Colin. J. Cats\"), \"W. B. Easy:\", clean_name(\"W. B. Easy\"))\n",
    "\n",
    "class Article:\n",
    "    def __init__(self, first_author, all_names, year, month, title, journal):\n",
    "        # clean the author names\n",
    "        # - remove non-ascii whitespace\n",
    "        # - strip bookend whitespace\n",
    "        # - strip periods from first names if not an initialed name\n",
    "        \n",
    "        \n",
    "        self.first_author = clean_name(first_author)\n",
    "        self.names = [clean_name(name) for name in all_names]\n",
    "        self.year = year\n",
    "        self.month = month\n",
    "        self.title = title\n",
    "        self.journal = journal\n",
    "        \n",
    "        # create a unique identifier for this article\n",
    "        self.id = \"_\".join([journal, year, month, \"_\".join(title.translate(str.maketrans('', '', string.punctuation)).split(\" \"))])\n",
    "        \n",
    "        # determine if article has initialed names\n",
    "        self.has_initials = is_initialed_name(first_author)\n",
    "    \n",
    "    def last_name_set(self):\n",
    "        # return a set() of all the last names\n",
    "        name_set = set()\n",
    "        for name in self.names:\n",
    "            name_set.add(name.split(\" \")[-1])\n",
    "        return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"has_initials\"] = self.has_initials\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def article_from_map(article_map):\n",
    "    return Article(\n",
    "        first_author=article_map[\"first_author\"],\n",
    "        all_names=article_map[\"all_names\"],\n",
    "        year=article_map[\"year\"],\n",
    "        month=article_map[\"month\"],\n",
    "        title=article_map[\"title\"],\n",
    "        journal=article_map[\"journal\"])\n",
    "\n",
    "test_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"A. Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2019\",\n",
    "    month=\"02\",\n",
    "    title=\"a story of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "\n",
    "print(\"last name set:\", test_article.last_name_set())\n",
    "print(\"article id:\", test_article.id)\n",
    "print(\"article has initial:\", test_article.has_initials)\n",
    "print(\"article map:\", test_article.to_map())\n",
    "\n",
    "\n",
    "class Guessed:\n",
    "    def __init__(self, primary_article, guessed_names, match_article_id):\n",
    "        self.first_author = primary_article.first_author\n",
    "        self.names = primary_article.names\n",
    "        self.year = primary_article.year\n",
    "        self.month = primary_article.month\n",
    "        self.title = primary_article.title\n",
    "        self.journal = primary_article.journal\n",
    "        \n",
    "        # create a unique identifier for this article\n",
    "        self.id = \"_\".join([self.journal, self.year, self.month, \"_\".join(self.title.translate(str.maketrans('', '', string.punctuation)).split(\" \"))])\n",
    "        \n",
    "        # determine if article has initialed names\n",
    "        self.has_initials = True in [is_initialed_name(n) for n in self.names]\n",
    "        \n",
    "        self.guessed_names = guessed_names\n",
    "        self.match_article_id = match_article_id\n",
    "    \n",
    "    \n",
    "    def last_name_set(self):\n",
    "        # return a set() of all the last names\n",
    "        name_set = set()\n",
    "        for name in self.names:\n",
    "            name_set.add(name.split(\" \")[-1])\n",
    "        return name_set\n",
    "    \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"first_author\"] = self.first_author\n",
    "        m[\"all_names\"] = self.names\n",
    "        m[\"year\"] = self.year\n",
    "        m[\"month\"] = self.month\n",
    "        m[\"title\"] = self.title\n",
    "        m[\"journal\"] = self.journal\n",
    "        m[\"id\"] = self.id\n",
    "        m[\"has_initials\"] = self.has_initials\n",
    "        m[\"guessed_names\"] = self.guessed_names\n",
    "        m[\"match_article_id\"] = self.match_article_id\n",
    "        return m\n",
    "        \n",
    "        \n",
    "def guessed_from_map(guessed_map):\n",
    "    g = Guessed(\n",
    "        primary_article=article_from_map(guessed_map),\n",
    "        guessed_names=guessed_map[\"guessed_names\"],\n",
    "        match_article_id=guessed_map[\"match_article_id\"])\n",
    "    return g\n",
    "\n",
    "test_primary_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"A. Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2019\",\n",
    "    month=\"02\",\n",
    "    title=\"a story of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "test_match_article = Article(\n",
    "    first_author=\"cat\",\n",
    "    all_names=[\"Arthur Cat\", \"Dog\", \"Another Cat\", \"More Cats\"],\n",
    "    year=\"2018\",\n",
    "    month=\"05\",\n",
    "    title=\"existence of cool/cats\",\n",
    "    journal=\"GeoCatography\"\n",
    ")\n",
    "test_guessed = Guessed(test_primary_article, \"Arthur Cat\", test_match_article.id)\n",
    "\n",
    "print(test_guessed.to_map())\n",
    "print(guessed_from_map(test_guessed.to_map()).to_map() == test_guessed.to_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page parse functions\n",
    "\n",
    "def get_parse_function(filename):\n",
    "    if filename.startswith(\"JGR\") or filename.startswith(\"GRL\") or filename.startswith(\"G3\"):\n",
    "        return parse_agu_page\n",
    "    if filename.startswith(\"Seismological+Research+Letters\") or filename.startswith(\"Bulletin+of+the+Seismological+Society+of+America\"):\n",
    "        return parse_gsw_page\n",
    "    if filename.startswith(\"NatureGeoscience\") or filename.startswith(\"Nature\"):\n",
    "        return parse_ng_page\n",
    "    if filename.startswith(\"E%26PSL\") or filename.startswith(\"PEPI\") or filename.startswith(\"Tectp\"):\n",
    "        return parse_sd_page\n",
    "    if filename.startswith(\"GJI\"):\n",
    "        return parse_gji_page\n",
    "    if filename.startswith(\"SolidEarth\"):\n",
    "        return parse_solidearth_page\n",
    "    if filename.startswith(\"GEOPHYSICS\"):\n",
    "        return parse_geophysics_page\n",
    "    if filename.startswith(\"Science\"):\n",
    "        return parse_science_page\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_agu_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"item__body\")\n",
    "    for a in articles:\n",
    "        meta_title = a.find_all(class_=\"meta__title\")\n",
    "        title = meta_title[0].find_all(\"a\", class_=\"publication_title\")\n",
    "        title = title[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "       \n",
    "        if title == \"Issue Information\":\n",
    "            print('hit \"Issue Information\"')\n",
    "            continue\n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "\n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"a\", class_=\"publication_contrib_author\")\n",
    "        for p in author_spans:\n",
    "            author_span = p.span\n",
    "            if author_span.i is not None:\n",
    "                author_span.i.decompose()\n",
    "            authors.append(str(author_span.string))\n",
    "   \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "\n",
    "        article = Article(\n",
    "                first_author=authors[0],\n",
    "                all_names=authors,\n",
    "                year=year,\n",
    "                month=month,\n",
    "                title=title,\n",
    "                journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "\n",
    "def parse_gsw_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"al-article-box\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"al-title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"wi-fullname\")\n",
    "        for author in author_spans:\n",
    "            author = str(author.find_all(\"a\")[0].get_text())\n",
    "            authors.append(author)\n",
    "          \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_ng_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"li\", class_=\"pb20\")\n",
    "\n",
    "    for a in articles:\n",
    "        title_section = a.find_all(\"h2\", class_=\"h3\")\n",
    "        if title_section[0] is None:\n",
    "            print(\"title section is none:\", a)\n",
    "            continue\n",
    "        title = title_section[0].a.contents[0]\n",
    "        \n",
    "        if title == None:\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "        title = title.strip()\n",
    "        \n",
    "        \n",
    "        authors = []\n",
    "        author_span = a.find_all(\"ul\", class_=\"js-list-authors-3\")\n",
    "        for auths in author_span:\n",
    "            for author in auths.find_all(\"a\", class_=\"js-no-scroll\"):\n",
    "                author = author.contents[0]\n",
    "                \n",
    "                if author == \"[…]\" or \"Show fewer authors\" in author:\n",
    "                    continue\n",
    "                \n",
    "                authors.append(author)\n",
    "                \n",
    "                \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "        \n",
    "    return parsed_articles\n",
    "\n",
    "        \n",
    "def parse_sd_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"col-sm-12\")\n",
    "\n",
    "    for a in articles:\n",
    "        title = a.find_all(\"h3\", class_=\"s-results-title\")\n",
    "        if title == \"None\":\n",
    "            #print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            #print(\"hit empty title\")\n",
    "            continue\n",
    "        title = title[0]\n",
    "        title = str(title.get_text())\n",
    "\n",
    "        authors = []\n",
    "        author_field = a.find(\"ul\", class_=\"all-authors\")\n",
    "        if author_field is None:\n",
    "            continue\n",
    "        author_list = author_field.find_all(\"li\", class_=\"article-author\")\n",
    "        for p in author_list:\n",
    "            author = p.get_text().strip(\";\")\n",
    "            name = author.split(\",\")[::-1]\n",
    "            name = \" \".join(name).strip()\n",
    "            authors.append(str(name))\n",
    "   \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "\n",
    "def parse_solidearth_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"paperlist-object\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(\"a\", class_=\"article-title\") \n",
    "        title = title[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title: \", a)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        authors = []\n",
    "        try:\n",
    "            author_spans = a.find_all(\"div\", class_=\"authors\")[0]\n",
    "        except IndexError:\n",
    "            print(\"hit empty authors.\")\n",
    "            continue\n",
    "    \n",
    "        for author in author_spans.string.split(\",\"):\n",
    "            author = str(author).strip()\n",
    "            if author[0:3] == \"and\":\n",
    "                author = author[3:].strip()  \n",
    "            if \" and \" in author:\n",
    "                #print(author)\n",
    "                tmp = author.split(\" and \")\n",
    "                authors.append(tmp[0])\n",
    "                authors.append(tmp[1])\n",
    "            else:\n",
    "                authors.append(author)\n",
    "\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_geophysics_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"issue-item__body\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"issue-item__title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "            \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find(\"div\", class_=\"issue-item__authors\")\n",
    "        a = author_spans.find_all(\"a\")\n",
    "        for auth in a:\n",
    "            author = str(auth.string)\n",
    "            authors.append(author)\n",
    "            \n",
    "        \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_gji_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"al-article-box\")\n",
    "    for a in articles:\n",
    "        title = a.find_all(class_=\"al-title\")\n",
    "        title = title[0].find_all(\"a\")[0]\n",
    "        if isinstance(title, NavigableString):\n",
    "            title = str(title.string)\n",
    "        if isinstance(title, Tag):\n",
    "            title = str(title.get_text())\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue        \n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"div\", class_=\"sri-authors al-authors-list\")\n",
    "        for au_span in author_spans:\n",
    "            aus = au_span.find_all(\"a\")\n",
    "            for au in aus:\n",
    "                authors.append(str(au.get_text()))\n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "    return parsed_articles\n",
    "\n",
    "def parse_science_page(soup, year, month, journal):\n",
    "    parsed_articles = []\n",
    "    \n",
    "    articles = soup.find_all(\"div\", class_=\"results-cit\")\n",
    "\n",
    "    for a in articles:\n",
    "        title_span = a.find(\"span\",  class_=\"cit-first-element\")\n",
    "        title = str(title_span.text)\n",
    "        \n",
    "        if title == \"None\":\n",
    "            print(\"hit NoneType title\")\n",
    "            continue\n",
    "        if len(title) == 0:\n",
    "            print(\"hit empty title, \", a)\n",
    "            continue\n",
    "        \n",
    "        authors = []\n",
    "        author_spans = a.find_all(\"span\", class_=\"cit-auth\")\n",
    "        for au_span in author_spans:\n",
    "            au = au_span.text.strip()\n",
    "            if au != \"and\":\n",
    "                authors.append(au)\n",
    "                \n",
    "            \n",
    "        if len(authors) == 0:\n",
    "            print(\"hit empty authors. title:\", title)\n",
    "            continue\n",
    "    \n",
    "        article = Article(\n",
    "            first_author=authors[0],\n",
    "            all_names=authors,\n",
    "            year=year,\n",
    "            month=month,\n",
    "            title=title,\n",
    "            journal=journal)\n",
    "        parsed_articles.append(article)\n",
    "\n",
    "\n",
    "    return parsed_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create one json per article with article info, stored in parsed/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk directory, parse, and save parsed articles\n",
    "\n",
    "for _, _, files in os.walk(RAW_PAGES_DIR):\n",
    "    for file in files:\n",
    "        \n",
    "#         # CHANGE THIS TO FILTER FOR SPECIFIC JOURNALS\n",
    "        #if (file.startswith(\"JGR\") or file.startswith(\"GRL\") or file.startswith(\"G3\")): \n",
    "        #    continue\n",
    "               \n",
    "        parser = get_parse_function(file)\n",
    "        if parser is None:\n",
    "            print(\"got None parse function, skipping file:\", file)\n",
    "            continue\n",
    "        #print(\"processing file:\", file)\n",
    "        print(file[0:3], end=\",\")\n",
    "        journal, year, month, _ = file.split(\"_\")\n",
    "\n",
    "        html = \"\"\n",
    "        with codecs.open(RAW_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            html = infile.read()\n",
    "        \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        for article in parser(soup, year, month, journal):\n",
    "            outfile_name = article.id[:80]+\".json\"\n",
    "            with codecs.open(PARSED_PAGES_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "                outfile.write(json.dumps(article.to_map()))\n",
    "            \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine how many papers have initialed authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many documents have initialed first authors\n",
    "count = 0\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if not contains_initialed_name(art.names):\n",
    "            continue\n",
    "        #print(art.names)\n",
    "        count += 1\n",
    "\n",
    "        \n",
    "print(\"{count} papers from a total of {total} contain initialed author names.\".format(count = count, \n",
    "                                                                                   total = len(files)))\n",
    "\n",
    "# Create in-memory map for name guessing\n",
    "\n",
    "# This map is generated from the articles where names are not initialed.\n",
    "# The keys on the map are last names. The values are arrays of articles\n",
    "# where at least one author on the article has the keyed last name.\n",
    "\n",
    "\n",
    "last_names_to_articles = {}\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "            \n",
    "        # considering all authors and all articles:\n",
    "        for name in art.names:\n",
    "            # only add if it is not initialed\n",
    "            if is_initialed_name(name):\n",
    "                continue\n",
    "            \n",
    "            last_name = name.split(\" \")[-1]            \n",
    "            \n",
    "            if last_name not in last_names_to_articles.keys():\n",
    "                last_names_to_articles[last_name] = []\n",
    "            last_names_to_articles[last_name].append(art)\n",
    "        \n",
    "print(\"All last names, map size\", len(last_names_to_articles.keys()))\n",
    "\n",
    "# TODO: repeat the same for EGU abstracts\n",
    "\n",
    "PARSED_EGU_PAGES_DIR = root+\"/egu_parsed/\"\n",
    "\n",
    "last_names_to_abstracts = {}\n",
    "\n",
    "for _, _, files in os.walk(PARSED_EGU_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_EGU_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "            \n",
    "        # considering all authors and all articles:\n",
    "        for name in art.names:\n",
    "            # only add if it is not initialed\n",
    "            if is_initialed_name(name):\n",
    "                continue\n",
    "            \n",
    "            last_name = name.split(\" \")[-1]            \n",
    "            \n",
    "            if last_name not in last_names_to_abstracts.keys():\n",
    "                last_names_to_abstracts[last_name] = []\n",
    "            last_names_to_abstracts[last_name].append(art)\n",
    "        \n",
    "print(\"All last names, abstract map size\", len(last_names_to_abstracts.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now try to guess their names by comparing to non-initialed authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create guesses for first author names and save to files\n",
    "def extract_name_guess(initial_name, possible_names):\n",
    "    last_name = initial_name.split(\" \")[-1]\n",
    "    for pn in possible_names:\n",
    "        if last_name != pn.split(\" \")[-1]:\n",
    "            continue  # not the name we're looking for\n",
    "        if (initial_name[0] != pn[0]) and (fuzz.token_sort_ratio(initial_name,pn) < 90):\n",
    "            continue  # first letters do not match TODO: There are cases where this is not correct, e.g.: Martin Mai and P. Martin Mai\n",
    "        if len(initial_name.split()) > 2 and len(pn.split()) > 2:\n",
    "            # both names have middle initial\n",
    "            if initial_name.split()[1][0] != pn.split()[1][0]:\n",
    "                # middle initial does not fit\n",
    "                continue\n",
    "        if len(initial_name.split(\"-\")) > 1 and len(pn.split(\"-\")) > 1:\n",
    "            # both names have hyphen\n",
    "            if initial_name.split(\"-\")[1][0] != pn.split(\"-\")[1][0]:\n",
    "                # second part of hyphenated name does not fit\n",
    "                continue\n",
    "        if is_initialed_name(pn):\n",
    "            continue\n",
    "        return pn, True\n",
    "    return \"\", False\n",
    "\n",
    "# create an output text file to quickly check if any bullshit occurs\n",
    "check_file = open(\"output_checklist_guessednames.txt\", \"w\")\n",
    "\n",
    "\n",
    "# unsupported edge cases\n",
    "# Check that the same initialized name\n",
    "# is not mapped to multiple different\n",
    "# complete names\n",
    "# J. M. Li => Jia Li\n",
    "# J. M. Li => Jiaxun Li\n",
    "# J. M. Li => Jingyuan Li\n",
    "\n",
    "count = 0\n",
    "unmapped_names = set()\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if not contains_initialed_name(art.names): #better contains initials\n",
    "            continue\n",
    "        \n",
    "        # TODO in this whole cell:\n",
    "        # adapt so that all authors are guessed, not only the first author\n",
    "        \n",
    "        guessed_authors = []\n",
    "        \n",
    "        for author in art.names:\n",
    "            \n",
    "            if not is_initialed_name(author): \n",
    "                guessed_authors.append(author)\n",
    "                continue\n",
    "                \n",
    "            last_name = author.split(\" \")[-1]\n",
    "\n",
    "            if last_name not in last_names_to_articles.keys() and\\\n",
    "            last_name not in last_names_to_abstracts.keys():\n",
    "                guessed_authors.append(author)\n",
    "                unmapped_names.add(author)\n",
    "                # author is neither in articles nor abstracts\n",
    "                count += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                articles = last_names_to_articles[last_name]\n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                abstracts = last_names_to_abstracts[last_name]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            # gather all the guesses, paired with the size of overlap.\n",
    "            # guesses with largest overlap will be written to file. \n",
    "            guesses = []\n",
    "            possible_names = set()\n",
    "            \n",
    "    \n",
    "            for article in articles + abstracts:\n",
    "                overlap = article.last_name_set() & art.last_name_set()\n",
    "                # TODO: I have a doubt here that this also compares to the article that we are trying to guess itself?\n",
    "                # No, because if the name has initials it does not go to last_names_to_articles dictionary\n",
    "                # please check\n",
    "                if (len(art.names) > 1) and (len(overlap) < 2):\n",
    "                    name, ok = extract_name_guess(author, article.names)\n",
    "                    if ok:\n",
    "                        possible_names.add(name.split(\" \")[0] + \" \" + name.split(\" \")[-1])\n",
    "                    continue  # skip articles without enough overlap. TODO: we have a problem for authors that write alone\n",
    "            \n",
    "                # TODO Adapt this so that all authors are added to guessed names\n",
    "                # not only the first author\n",
    "                name, ok = extract_name_guess(author, article.names)\n",
    "                if not ok:\n",
    "                    continue\n",
    "            \n",
    "                check_file.write(\"{}\\t\\t{}\\n\".format(name, author))\n",
    "                guessed = Guessed(art, [name], article.id)\n",
    "                guesses.append((len(overlap), guessed))\n",
    "            \n",
    "\n",
    "                    \n",
    "            #simplify set of possible names due to slightly different characters\n",
    "            \n",
    "            tmp = list(possible_names)\n",
    "            tmp2 = []\n",
    "\n",
    "            for i in range(len(possible_names)-1):\n",
    " \n",
    "                highest = process.extractOne(tmp[i],tmp[i+1:])\n",
    "                if highest[1] > 85:\n",
    "                    continue\n",
    "                tmp2.append(tmp[i])\n",
    "\n",
    "            if len(tmp)!=0:\n",
    "                tmp2.append(tmp[-1])\n",
    "\n",
    "            possible_names = set(tmp2)   \n",
    "\n",
    "                \n",
    "        \n",
    "            if len(guesses) == 0:\n",
    "                if len(possible_names)==1:\n",
    "                    check_file.write(\"{}\\t\\t{}\\n\".format(list(possible_names)[0], author))\n",
    "                    guessed = Guessed(art, [list(possible_names)[0]], article.id)\n",
    "                    guesses.append((len(overlap), guessed))\n",
    "                else:# maybe another if: if all possibilities are male names, take whatever (Tim and Timothy are not distinguished)              \n",
    "                    guessed_authors.append(author)\n",
    "                    unmapped_names.add(author)\n",
    "                    count += 1\n",
    "                    continue\n",
    "\n",
    "            guesses = sorted(guesses, key=lambda x: x[0], reverse=True) #This does not change guesses if we do not assign it to variable guesses\n",
    "            the_guess = guesses[0][1]\n",
    "            guessed_authors.append(the_guess.guessed_names[0])\n",
    "            \n",
    "            \n",
    "        final_guess = Guessed(art, guessed_authors, the_guess.match_article_id)    \n",
    "        outfile_name = final_guess.id[:80]+\".json\"\n",
    "        with codecs.open(GUESSED_NAMES_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(final_guess.to_map()))\n",
    "\n",
    "check_file.close()\n",
    "print('Names not guessed:',len(unmapped_names))\n",
    "print('Number of articles not guessed:', count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start with the name --> gender: test genderize.io api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for gender result and test\n",
    "class GenderResult(object):\n",
    "    def __init__(self, name, result):\n",
    "        self.name = name\n",
    "        self.binary = result[0]\n",
    "        self.percent = result[1]\n",
    "        self.count = result[2]\n",
    "        \n",
    "    def to_map(self):\n",
    "        m = {}\n",
    "        m[\"name\"] = self.name\n",
    "        m[\"binary\"] = self.binary\n",
    "        m[\"percent\"] = self.percent\n",
    "        m[\"count\"] = self.count\n",
    "        return m\n",
    "    \n",
    "\n",
    "def gender_result_from_map(m):\n",
    "    return GenderResult(m[\"name\"], ( m[\"binary\"], m[\"percent\"], m[\"count\"]))\n",
    "\n",
    "name = \"Ismael\"\n",
    "g = getGenders(name)[0]\n",
    "gr = GenderResult(name, g)\n",
    "gender_result_from_map(json.loads(json.dumps(gr.to_map()))).to_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just read in all the previously determined and stored name --> gender pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the Gender API gender guesses into memory\n",
    "\n",
    "gender_results = []\n",
    "\n",
    "for _, _, files in os.walk(NAME_GENDER_DIR):\n",
    "    for file in files:\n",
    "        with codecs.open(NAME_GENDER_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            gr = gender_result_from_map(json.loads(infile.read()))\n",
    "            gender_results.append(gr)\n",
    "            \n",
    "        \n",
    "    \n",
    "# Create a map of first name to gender result\n",
    "name_to_gr = {}\n",
    "for gr in gender_results:\n",
    "    name_to_gr[gr.name] = gr\n",
    "    \n",
    "##print(\"Length of current name-gender map\", len(name_to_gr))\n",
    "\n",
    "# test\n",
    "print(\"Colin.\" in name_to_gr)\n",
    "print(\"First name in current gender map, gender: \", \\\n",
    "      list(name_to_gr)[0], name_to_gr[list(name_to_gr)[0]].binary)\n",
    "print(\"Last name in current gender map, gender: \", \\\n",
    "      list(name_to_gr)[-1], name_to_gr[list(name_to_gr)[-1]].binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genderize all the names\n",
    "#### - collect the list of first names from PARSED and GUESSED\n",
    "#### - check if it is in the list already, if not: call genderize io\n",
    "#### - finally check how many in the guessed initialed names are male / female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect list of all PARSED and GUESSED names to be gendered\n",
    "\n",
    "first_names = set()\n",
    "\n",
    "\n",
    "## TODO:  Adapt here for all authors instead for first author only\n",
    "files_guessed = glob(os.path.join(GUESSED_NAMES_DIR, \"*.json\"))\n",
    "files_parsed = glob(os.path.join(PARSED_PAGES_DIR, \"*.json\"))\n",
    "\n",
    "for file in files_guessed:\n",
    "    with codecs.open(file, \"r\", \"utf8\") as infile:\n",
    "        guess = guessed_from_map(json.loads(infile.read()))\n",
    "        \n",
    "    for name in guess.guessed_names:\n",
    "        first_name = name.split(\" \")[0]\n",
    "        if len(first_name) == 0:\n",
    "            print(\"empty name?:\", name)\n",
    "            continue\n",
    "        first_names.add(first_name)\n",
    "\n",
    "\n",
    "for file in files_parsed:\n",
    "    with codecs.open(file, \"r\", \"utf8\") as infile:\n",
    "        art = article_from_map(json.loads(infile.read()))\n",
    "    if art.has_initials:\n",
    "        continue\n",
    "        \n",
    "    for name in article.names:\n",
    "        first_name = name.split(\" \")[0]\n",
    "        if len(first_name) == 0:\n",
    "            print(\"empty name?:\", name)\n",
    "            continue\n",
    "        first_names.add(first_name)\n",
    "\n",
    "print(\"number of unique first names of all authors to guess:\", len(first_names))\n",
    "print(\"From {} to {}.\".format(list(first_names)[0],\n",
    "                              list(first_names)[-1]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Gender API and save output for all the GUESSED names\n",
    "# WARN: Makes API calls\n",
    "# Does nothing if the guessed names were genderized previously\n",
    "\n",
    "print(\"starting size:\", len(name_to_gr))\n",
    "\n",
    "for i, name in enumerate(first_names):\n",
    "    # print(i, end=\",\")\n",
    "    if name in name_to_gr:\n",
    "        continue\n",
    "    else:\n",
    "        result = getGenders(name)\n",
    "        if len(result) > 1:\n",
    "            print(\"long result:\", result)\n",
    "        r = result[0]\n",
    "        gr = GenderResult(name, r)\n",
    "        \n",
    "        # update gender name map in memory\n",
    "        name_to_gr[gr.name] = gr\n",
    "        \n",
    "        # also save for later use\n",
    "        file_name = \"_\".join(name.translate(str.maketrans('', '', string.punctuation)).split(\" \")) + \".json\"\n",
    "        with codecs.open(NAME_GENDER_DIR+file_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(gr.to_map()))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "print(\"finish size:\", len(name_to_gr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist in the guessed names\n",
    "# NOTE: This ratio should be for all guesses, not the reduced set() of names\n",
    "\n",
    "binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "\n",
    "for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "    for file in files:\n",
    "\n",
    "        with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            guess = guessed_from_map(json.loads(infile.read()))\n",
    "        first_name = guess.guessed_names[0].split(\" \")[0]\n",
    "        if len(first_name) == 0:\n",
    "            print(\"empty first name?:\", guess.guessed_names[0])\n",
    "            continue\n",
    "        try:\n",
    "            gr = name_to_gr[first_name]\n",
    "            binary_ratio_first_names[gr.binary] += 1\n",
    "        except KeyError:\n",
    "            print(\"not found in map:\", first_name)\n",
    "        \n",
    "        \n",
    "binary_ratio_first_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Now, let's create a json database that can be used in another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go both through the guessed names and the non-initialed names.\n",
    "# Find genders on the base of the name_to_gr map and save the results\n",
    "\n",
    "# While we do not yet have a name database large enough to go through all the names,\n",
    "# just go through a few for testing purposes:\n",
    "nfile = 100\n",
    "\n",
    "AUTHOR_GENDER_DIR = root + \"/author_genders/\"\n",
    "if not os.path.exists(AUTHOR_GENDER_DIR):\n",
    "    os.mkdir(AUTHOR_GENDER_DIR)\n",
    "\n",
    "for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "    for file in files[0:nfile]:       \n",
    "        with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if art.has_initials:\n",
    "            continue\n",
    "            \n",
    "        all_names = art.names\n",
    "        all_genders = []\n",
    "        all_percent = []\n",
    "\n",
    "        for n in all_names:\n",
    "            n = n.split()[0]\n",
    "            try:\n",
    "                all_genders.append(name_to_gr[n].binary)\n",
    "                all_percent.append(name_to_gr[n].percent)\n",
    "            except KeyError:\n",
    "                print(\"Name {} is not gendered yet\".format(n))\n",
    "                all_genders.append(None)\n",
    "                all_percent.append(None)\n",
    "        art_out = art.to_map()\n",
    "        art_out[\"all_genders\"] = all_genders\n",
    "        art_out[\"all_percent\"] = all_percent\n",
    "        #print(art_out[\"all_names\"])\n",
    "        print(art_out[\"all_genders\"])\n",
    "       \n",
    "        # save!\n",
    "        outfile_name = os.path.basename(file)\n",
    "        with codecs.open(AUTHOR_GENDER_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(art_out))\n",
    "# now do the guessed names\n",
    "for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "    for file in files[0: nfile]:       \n",
    "        with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "            art = article_from_map(json.loads(infile.read()))\n",
    "        if art.has_initials:\n",
    "            continue\n",
    "            \n",
    "        all_names = art.names\n",
    "        all_genders = []\n",
    "        all_percent = []\n",
    "\n",
    "        for n in all_names:\n",
    "            n = n.split()[0]\n",
    "            try:\n",
    "                all_genders.append(name_to_gr[n].binary)\n",
    "                all_percent.append(name_to_gr[n].percent)\n",
    "            except KeyError:\n",
    "                print(\"Name {} is not gendered yet\".format(n))\n",
    "                all_genders.append(None)\n",
    "                all_percent.append(None)\n",
    "        art_out = art.to_map()\n",
    "        art_out[\"all_genders\"] = all_genders\n",
    "        art_out[\"all_percent\"] = all_percent\n",
    "        print(art_out[\"all_genders\"])\n",
    "        # save!\n",
    "        outfile_name = os.path.basename(file)\n",
    "        with codecs.open(AUTHOR_GENDER_DIR+outfile_name, \"w\", \"utf8\") as outfile:\n",
    "            outfile.write(json.dumps(art_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following part may be moved to a new notebook. It is from the analysis of Pico et al. 2020-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist in the guessed names\n",
    "# printed out for each journal.\n",
    "\n",
    "\n",
    "journal_names = [\n",
    "\"Tectonophysics\",\n",
    "\"Physics+of+the+Earth+and+Planetary+Interiors\",\n",
    "\"Earth+and+Planetary+Science+Letters\",\n",
    "\"SolidEarth\",\n",
    "\"GEOPHYSICS\",\n",
    "\"NatureGeoscience\",\n",
    "\"GRL\",\n",
    "\"JGRSolidEarth\",\n",
    "\"G3\",\n",
    "\"GJI\",\n",
    "\"Nature\",\n",
    "\"Bulletin+of+the+Seismological+Society+of+America\",\n",
    "\"Seismological+Research+Letters\"]\n",
    "\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "    \n",
    "    for _, _, files in os.walk(GUESSED_NAMES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(GUESSED_NAMES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                guess = guessed_from_map(json.loads(infile.read()))\n",
    "            first_name = guess.guessed_name.split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", guess.guessed_name)\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                binary_ratio_first_names[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "    print(journal, binary_ratio_first_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many male/female/unknown names exist for the un-initialed names\n",
    "# printed out for each journal.\n",
    "\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0}\n",
    "    \n",
    "    initialed_name_count = 0\n",
    "    \n",
    "    for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                article = article_from_map(json.loads(infile.read()))\n",
    "            first_name = article.first_author.split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", article.first_author)\n",
    "                continue\n",
    "            if is_initialed_name(first_name):\n",
    "                initialed_name_count += 1\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                binary_ratio_first_names[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "    print(journal, binary_ratio_first_names, \"initialed name count:\", initialed_name_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but split into year/month bins\n",
    "#\n",
    "# Determine how many male/female/unknown names exist for the un-initialed names\n",
    "# printed out for each journal.\n",
    "\n",
    "journal_tallys = {}\n",
    "\n",
    "for journal in journal_names:\n",
    "    binary_ratio_first_names = {'male':0, 'female':0, 'None':0, 'initialed':0}\n",
    "    \n",
    "    initialed_name_count = 0\n",
    "    \n",
    "    for _, _, files in os.walk(PARSED_PAGES_DIR):\n",
    "        for file in files:\n",
    "            \n",
    "            if not file.startswith(journal):\n",
    "                continue\n",
    "                \n",
    "            with codecs.open(PARSED_PAGES_DIR+file, \"r\", \"utf8\") as infile:\n",
    "                article = article_from_map(json.loads(infile.read()))\n",
    "                \n",
    "            tally = journal_tallys.setdefault(journal, {}).setdefault(article.year, {}).setdefault(article.month, {'male':0, 'female':0, 'None':0, 'initialed':0})\n",
    "            \n",
    "            first_name = article.first_author.strip().split(\" \")[0]\n",
    "            if len(first_name) == 0:\n",
    "                print(\"empty first name?:\", article.first_author)\n",
    "                continue\n",
    "            if is_initialed_name(first_name):\n",
    "                tally['initialed'] += 1\n",
    "                continue\n",
    "            try:\n",
    "                gr = name_to_gr[first_name]\n",
    "                tally[gr.binary] += 1\n",
    "            except KeyError:\n",
    "                print(\"not found in map:\", first_name)\n",
    "\n",
    "\n",
    "for journal, values in journal_tallys.items():\n",
    "    print(journal, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
